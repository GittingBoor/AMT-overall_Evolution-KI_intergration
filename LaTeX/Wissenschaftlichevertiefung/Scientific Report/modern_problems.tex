\section{Hindernisse Moderner AMT-Systeme}
Trotz der Einführung von KI-Modulen gibt es immer noch zahlreiche offene Probleme, die noch nicht gelöst worden
oder noch nicht perfekt gelöst sind.
Zudem agieren KIs völlig anders als normale Algorithmen.
Dies führte, zum Teil, auch zu neuen Problemen, welche davor nicht mal bekannt waren.
In den folgenden Abschnitten werden die größten und wichtigsten Probleme,
mit der sich die AMT Forschung momentan auseinandersetzt, aufgezählt.

\subsection{Onset Detection}
Die On-/Offset erkennung von Noten wurde schon ausgiebig in zahlreichen Arbeiten behandelt.
Trotzdem ist diese noch nicht komplett akkurat.
Das liegt daran, dass man On-/Offsets an Lautstärke sprüngen und spektralen Änderungen erkennt.
Bei polyphonen Musikstücken mit vielen verschiedenen Noten kann man jedoch schwieriger diese Unterschiede erkennen.
Lautstärke sprünge werden ungenauer, da häufig mehrere Noten während des Onsets einer bestimmten Note spielen.
Mit spektralen Änderungen sind hier Veränderungen der Energieverteilung gemeint.
Einer der ausschlaggebendsten Anteile ist die Spektrale Fluktuation.
Diese stellt den plötzlichen Anstieg von Energie in bestimmten Frequenzbändern dar.
Wenn nun ein Ton auf einem Klavier gespielt wird kann somit der Onset sehr gut ermittelt werden.
Bei Instrument wie einer Geige kann dies jedoch zu problemen führen.
Hier können Noten gebunden gespielt werden, was zu einem unterschied in der Frequenz,
jedoch nicht in dem Energie level führt.
Zudem kann eine Note, durch Crescendo, zunächst leise gespielt werden und mit der Zeit an Lautstärke zunehmen.
Dieser Onset hat keinen Energie-Peak und somit erkennt das System hier auch nur schwierig eine scharfe Kante.
So welche Töne, welche keinen starken Einschlag haben, werden als nicht-perkussiv bezeichnet.
Ein weiteres Problem der Onset erkennung sind nach wirkende Geräusche oder stör Geräusche.
Bleiben wir bei dem Beispiel einer Geige, so können einige Töne, sobald man aufhört andere Töne zu spielen, nachklingen.
Dadurch könnten Töne von anderen Instrumenten verdeckt werden, wodurch das System den Onset nicht erkennt.
Ähnlich kann dies auch der Fall sein, wenn im Audiosignal ein starkes Rauschen besteht.

\subsection{Polyphonie}
Durch die nutzung von polyphonen Musikstücken sind, wie schon bei der Onset Erkennung,
einige Probleme schwerer und deutlicher geworden.
Ein weiteres Problem, das gezielt von diesen Anwendungsfällen abhängt, ist die Verwechselung von Noten im Audiosignal.
Einige Noten haben sehr ähnliche Frequenzen.
Wenn diese Noten gleichzeitig gespielt werden, ist es schwieriger für das System, die korrekten Noten herauszuhören.
Neuronale Netzwerke helfen hierbei deutlich, da diese nicht nur das Spektrum zur analyse einbeziehen,
sondern auch auf einer zeitlichen und harmonischen Ebene den Kontext besser zuordnen können
und somit die wahrscheinlichsten nächsten Noten zurückgeben können.
Trotz dessen scheitert auch KI an der einordnung der richtigen Noten, wenn die gespielten Noten
eine sehr ähnliche Frequenz besitzen oder die Akkorde zu unterschiedlich zu den Trainingsdaten sind.
In der folgenden Arbeit werden diese Probleme nochmals deutlicher aufgegriffen.
\cite{martak2022balancing}

\subsection{Spezifische Instrument}
Oft ist die Qualität der Transkription auch abhängig von den vertretenen Instrumenten in dem Audiosignal.
Ethnische Instrumente zum Beispiel sind Instrumente die einer bestimmten Kultur angehören
und in der westlichen Kultur weniger vertreten sind.
Dadurch gibt es auch weniger Datensätze, welche diese Instrumente beinhalten.
KIs brauchen Trainingsdaten und ohne diese können sie bestimmte Instrumente nicht richtig zuordnen.
Die meisten Datensätze zur Musiktranskription bestehen hauptsächlich aus Klavier und Geigen Noten.
Instrumente wie Flöten oder Orgeln können von diesen Noten gut abgeleitet werden,
da sich deren Struktur deutlich ähnelt.
Eine weitere Gruppe von Instrumenten die Probleme bei der Transkription bereitet sind elektronische Instrumente.
Wenn man zum Beispiel eine E-Gitarre spielt, kann diese Effekte nutzen,
welche nicht üblich in klassischen Datensätzen von Klavieren vorkommen.
Somit erkennt die KI diese Töne nicht korrekt an.
Das letzte Instrument welches schwierigkeiten bringt, ist der Gesang.
Jede Stimme ist einzigartig und vor allem nicht statische.
Wenn man einen Ton auf dem Klavier spielt, besitzt dieser,
wenn das Klavier richtig gestimmt ist, immer die gleiche Frequenz.
Ein Mensch kann jedoch nicht jeden Ton immer komplett perfekt spielen, wodurch eine große varianz an Tönen entsteht.
Zudem verläuft der Klang einer Stimme von einer Note zur nächsten.
Es gibt nicht immer starke Peaks zur Onset Erkennung.
Gesang ist auch, wie die vorher genannten Instrumentgruppen, nicht sonderlich vertreten in größeren Datensätzen.
Die folgenden Paper konzentrieren ihren Fokus speziell auf das Thema des Gesangs in der Musiktranskription.
\cite{gu2023deep,gu2024automatic}
Somit wurde eine Onset Erkennungsgenauigkeit von ungefähr 80\% festgestellt, für Gesang.

\subsection{Notendauer}
Neben dem Onset einer Note muss man auch erkennen, wie lange eine bestimmte Note gespielt wird.
Das kann schwierig sein, da man den Nachhall einer Note von der wirklich gespielten Zeit der Note trennen muss.
Es gibt jedoch bei einigen Instrumenten, wie zum Beispiel dem Klavier, ein spezielles Problem.
Wenn man während des Spielens einer Note das Pedal drückt, gibt es keinen klaren Punkt,
an dem man den Übergang zwischen der Note und dem Nachhallen eindeutig erkennen kann.
Die Lautstärke sinkt dabei nicht abrupt, sondern nur langsam ab.
Natürlich ist dieses Problem in polyphonen Musikstücken nochmal deutlich stärker,
da sich dort die verschiedenen Nachhallphasen überlappen.
Dies ist eins der grundlegendsten Probleme, zusammen mit der Onset Erkennung.
\cite{jamshidi2024machine}

\subsection{Reale Audioaufnahmen}
Ein perfektes AMT-System sollte sogar auf realen Audioaufnahmen 100\% Genauigkeit besitzen.
In der Realität klappt das aber nicht so gut, da Live-Audioaufnahmen mehr Hintergrundrauschen besitzt.
Dies war schon vor der Einführung von KI ein Problem und wurde durch die Einbindung
von KI-Modulen nicht sonderlich viel verbessert.
Das liegt daran, dass KIs mit isolierten Studioaufnahmen, wie es im MAESTRO Datensatz der Fall ist, trainiert werden.
Natürlich gibts auch Datensätze mit realen Audioaufnahmen, jedoch bräuchte man dafür viel mehr Trainingsdaten und
vor allem auch eine große Anzahl von unterschiedliche Hintergrundgeräusche,
damit die KI auf alle Möglichkeiten trainiert wird.
Um dem entgegenzuwirken, kann man Studio-Datensätze wie MAESTRO durch Data-Augmentation variieren,
wodurch realistischere Audioaufnahmen entstehen.
So wurde es auch in folgender Arbeit, für die Trainingsdaten, eingesetzt.
\cite{kusaka2024mobile}
Es wird Timber, Reverb, Noise variierte und die Qualität des Aufnahmegerätes, zu der eines Smartphones, veränderte.
So bekommt man zahlreiche neue Datensätze, die passend auf die gegebenen Anwendungsfall ausgelegt sind.

\subsection{Frame-basierte vs Event-basierte AMT-Systeme}
Die meisten AMT-Systeme sind Frame-basiert und nicht Event-basiert.
Frame-basiert bedeutet, das die Analyse des Audiosignals in Zeitfenster, ungefähr 20ms, aufgeteilt wird.
Jedes Zeitfenster wird somit der momentane Stand das Audiosignal von den verschiedenen Modulen analysiert.
Falls nun zum Beispiel ein Onset einer Note genau zwischen zwei Zeitfenstern liegt,
wird dieser verschoben oder verzerrt, sodass dieser mit den gegebenen Zeitfenstern übereinstimmt.
Event-basierte AMT-Systeme hingegen fragen immer ab,
was das nächste musikalische Ereignis im Audiosignal ist und reagieren dementsprechend.
Dadurch entspricht der Output viel mehr dem Input, in relation zu der zeitlichen Abfolge.
Im Ergebnis sind Event-basierte AMT-Systeme somit Frame-basierten AMT-Systemen überlegen.
Jedoch ist in der Realität trotzdem fast jedes AMT-System Frame-basiert.
Das liegt daran, dass Event-basierte AMT-Systeme eine sehr neue Entwickelung sind
und zudem auch schwerer zu implementieren ist.
Frame-basierte Methoden wurden ungefähr im Jahre 2000 entwickelt
und haben sich seitdem in zahlreichen Arbeiten durchgesetzt.
\cite{Martin1996, klapuri1998automatic}
Dahingegen sind Event-basierte Methoden erst ungefähr 15 Jahre später entwickelt worden.
\cite{performance_rnn2017event}
Deshalb wurden Frame-basierte Methoden lange als der Standard angesehen.
Zudem ist die Umsetzung von Event-basierten AMT-Systemen schwerer als Frame-basierte.
Das System muss korrekt jedes Event einschätzen un zuordnen, ist es ein Onset ein Offset oder doch nur Rauschen?
Wenn es nun einen Fehler macht, wird sich das,
im gegensatz zu Frame-basierten, auf das ganze weitere Audiosignal auswirken.
Auch das Training kann nicht parallel verlaufen, sondern muss token weise abgearbeitet werden.
Diese Gründe, und noch einige andere, machen Event-basierte AMT-Systeme zu einer viel größeren Herausforderung.
Jedoch ist der übergang von Frame zu Event mit der Zeit irgendwann nötig, da man somit den Inhalt der Musikstücke
viel präziser wiedergeben kann.
Die Forschung in dem Event-basierten Methoden steigt auch in den letzten Jahren immer mehr an.

\subsection{Blackbox von KI-Modellen}
Das letzte Problem betrifft KI im generellen.
Ein neuronales Netz arbeitet mit sehr hochdimensionalen Räumen, die für uns Menschen nicht begreifbar sind.
Selbst wenn jemand sich die Millionen und mehr Gewichtungen anschauen würde,
würde dieser in diesen keinen Zusammenhang feststellen können.
In der Musiktranskription ist dies auch ein Problem, da somit nicht erfasst werden kann,
welche Einstellungen eine automatischen Musiktranksriptions KI genau braucht,
damit sie alle Musikstücke perfekt transkribieren kann.
Jedoch gibt es auch Methoden, welche die Blackbox ein wenig umgehen.
Im Forschungsgebiet der Explainable AI gibt es einige Methoden,
welche auch in KI-basierten AMT-Systemen, verwendet werden.
Unter diesen fallen Saliency Maps, Feature-Visualisierung und Attention-Mapping.
Concept Activation Vectors, Layer-wise Relevance Propagation, Surrogat-Modelle sind andere Explainable AI Methoden,
die zur nutzung in AMT-Systemen diskutiert werden, aber noch nicht richtig integriert sind.
Dies liegt an mehreren Gründen.
Musikalische Konzepte wie Akkorde oder Onsets sind nicht direkt lablebar, wie zum beispiel Katzen oder Hunde,
da diese stark abhängig von spektralen und zeitlichen Mustern sind.
AMT-Systeme sind zeitlich-sequenziell und besitzen pro Zeitfenster mehrere Outputs.
Viele lokale Änderungen auf ein bestimmtes Zeitfenster, wie den dB-Wert ändern, haben einen globalen Einfluss.
Methoden, welche in AMT-Systemen angewendet werden,
haben dahingegen Eigenschaften, die durch die Struktur von AMT-Systemen profitieren.
Saliency Maps stellen dar, wie stark jedes Zeitfenster im Spektrogramm,
beeinflusst hat, das zum Beispiel ein bestimmter Ton erkannt wurde.
Dies basiert auf dem Gradienten und ist besonders hilfreich bei CNN-basierten Modellen.
Feature-Visualisierung beobachtet konkret die einzelnen Neuronen jeder Ebene im neuronalen Netzwerk.
Es wird geschaut, welche neuronen sehr stark bei bestimmten Inputs reagieren.
So kann man erkennen, welche Teile des Netzes für welche Aufgaben verantwortlich sind
und ob vielleicht bestimmte stellen des Netzes gar nicht genutzt werden.
Attention-Mapping nimmt den gegebenen Input und gewichtet diesen,
je nachdem welche Teile davon wichtig für eine bestimmte Vorhersage sind.
So erkennt man, welche Zeitfenster den meisten Einfluss auf zum Beispiel einen bestimmte Onset hatten.
Vor allem bei polyphonen Musikstücken kann man relevante zeitliche Zusammenhänge erkennen.
Ein gutes Beispiel von Attention-Mapping findet man in folgender Arbeit.
\cite{cheuk2021revisiting}
