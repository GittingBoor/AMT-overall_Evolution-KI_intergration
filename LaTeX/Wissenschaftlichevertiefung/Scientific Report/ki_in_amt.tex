\section{KI-Module in AMT-Systemen}
\label{sec:ki_integration}
KI-Systeme haben, in den letzten Jahren stark an beliebtheit gewonnen.
AMT-Systeme bilden da keine Ausnahmen.
Vor allem durch die integration von CNNs und RNNs konnten AMT-Systeme viele Prozesse deutlich verbessern
und neue Errungenschaften in dem Forschungsgebiet erzielen.
Es gibt jedoch auch weitere wichtige KI-Module die in AMT-Systemen häufig genutzt werden
oder zur integration in Planung stehen.
In diesem Kapitel werde ich auf genau diese KI-Module stärker eingehen
und deren Aufgaben in der automatischen Musik transkription weiter erläutern.

\subsection{Convolutional Neural Networks}
CNNs sind neuronale Netze, welcher besonders gut räumlich strukturierte Daten analysieren können.
Deshalb werden diese vor allem in der analyse von Bildern genutzt.
Sie können zum Beispiel erkennen, was auf einem Bild genau passiert oder welche Objekte in einem Bild zu erkennen sind.
Auch KIs wie ChatGPT nutzen eine verbesserte Form von CNNs, um Bilder zu analysieren.
Im Fall der Musiktranskription haben wir als Input Bild das Spektrogramm.
Spektrogramme können ähnliche wie zweidimensionale Bilder gehandhabt werden,
da auf diesen auch alle wichtigen Daten des Inputs Audiosignals zu finden sind.
CNNs bestehen aus mehreren verschiedenen Layern.
Einfache CNN Modelle bestehen nur aus 2 bis 5 Layern,
wobei komplexere CNNs aus über tausende Layern bestehen können.
In AMT-Systemen haben die meisten CNNs etwa drei bis zehn Layer.
Diese Layer sind Convolutional Layer (Faltungsschicht), Activation Layer (ReLU), Pooling Layer,
Batch Normalization, Dropout Layer und Upsampling.
Mit jedem Layer kann ein CNN immer abstraktere Merkmale erkennen.
Außer dem Convolutional Layer und Activation Layer sind die anderen Layer jedoch nicht unbedingt notwendig.
Eine Arbeit, welche die stärke, von CNN-Modellen in AMT-Systemen, sehr gut darstellt ist die von Curtis Hawthorne.
\cite{hawthorne2017onsets}
Er benutzt direkt zwei verschieden spezialisierte Teilnetzwerke für Onsets und Sustain der Noten.
Mit diesem System illustriert er die Entwickelung des Forschungsgebietes,
insbesondere für polyphone Klaviertranskription, ausgezeichnet.
Im Folgenden werde ich die verschiedenen Layer eines CNNs, in einem AMT-System, erläutern.

\subsubsection{Layer eines CNNs}
In dem Convolution Layer werden Filter verwendet.
Filter sind 2D-Matrizen, die aus trainierbaren Gewichten bestehen.
Ein Filter deckt jeweils einen 3x3 Pixel Eingabebereich des Inputbildes ab.
Jeden Filter, den man auf das Bild anwendet,
wird über das gesamte Bild gezogen und analysieren dadurch erstmal jeden Eingabebereich einzeln.
Ein Skalarprodukt aus Filter und Eingabebereich beschreibt dann einen Aktivierungswert.
Aus allen Aktivierungswerten eines Filters entsteht eine Feature Map.
Wenn man Aktivierungswerte miteinander vergleicht,
können somit Patterns und Eigenschaften erkannt werden.
In der Musiktranskription filtert man somit Onsets, Sustains oder harmonische Verläufe heraus.
Zum Beispiel Onsets werden erkannt, wenn es eine plötzliche Energieänderung gibt.
Am Ende bekommt man einen 3D-Tensor raus, welcher alle Feature Maps beinhaltet.

Die Batch Normalization sorgt dafür, das die Aktivierungswerte normalisiert werden.
Jede Feature Map wird dabei einzeln normalisiert.
Dadurch wird Rechenleistung eingespart.
Zudem kann man im Training durch Mini-Batches mehrere Spektrogramme gleichzeitig durch die CNN Struktur leiten.
Dadurch wird das Training schneller und man kommt früher an Ergebnisse.

Es kann passieren das die Summe eines Convolution Layers negativ ist.
Dies kann passieren, wenn stärker gewichtete Filter einen negativen Aktivierungswert herausgeben.
Negative Werte können zu Informationsverlust, von Eigenschaften des Musikstückes, führen.
Um das zu vermeiden werden im Activation Layer, meistens mit der ReLU Funktion,
alle negativen Aktivierungswerte auf 0 gesetzt.
So kann das Netz nichtlineare Beziehungen modellieren.

Als Nächstes wird mit dem Pooling Layer der Rechenaufwand verringert.
Dieser nimmt jede Feature Map einzeln und reduziert deren Matrix zu einer kleineren, meistens 2x2, Matrix.
Das erfolgt, indem sich der Pooling Layer zunächst eine
gesamte Feature Map nimmt und diese dann in kleinere Blöcke aufteilt.
Es gibt entweder Max-Pooling oder Average-Pooling.
Je nachdem welche Methode man wählt, wird immer der höchste Wert oder der durchschnittliche Wert extrahiert.
Der extrahierte Wert von jedem Block wird nun in die reduzierte Feature Map zurückgeführt.
Dadurch reduziert man nicht nur Rechenaufwand, sondern vermeidet auch überanpassung.
Wenn das System jeden kleinsten Wert berücksichtigt passt es sich zu sehr an den trainingsdaten an
und kann womöglich andere Daten nicht mehr richtig analysieren.

Der Dropout Layer ist, im gegensatz zu den anderen Layern, nur im Training relevant.
Er schaltet zufällig bestimmte Neuronen aus,
sodass sich Neuronen nicht ausschließlich auf bestimmte andere Neuronen verlassen können.
Somit wird das gesamte neuronale Netzwerk robuster und vielseitiger.

Upsampling ist das Gegenteil von dem Pooling Layer.
Anstatt die Feature Maps zu reduzieren, werden diese wieder hochskaliert.
Dadurch kann man bestimmte Features wieder zeitlich präziser bestimmen,
da die Zeitfenster wieder genauer zum originellen Audiosignal sind.
Meist wird dieser Layer jedoch weggelassen, da er meistens nicht sehr relevant für AMT-Systeme ist.

\subsubsection{Geschichte von CNNs}
CNNs finden Ihren Ursprung im Jahre 1979.
Kunihiko Fukushima erfand damals die Architektur für CNNs unter dem Namen Neocognitron.
\cite{fukushima1980neocognitron}
Dies sollte als Vorreiter für spätere CNN Modelle gelten.
Es dauerte weitere 10 Jahre bis das erste richtige Convolutional Neural Network, von Yann LeCun, veröffentlicht wurde.
\cite{lecun1989backpropagation}
Dieses CNN Modell unterscheidet sich speziell in drei bestimmten Punkten zu dem Vorgänger Neocognitron.
LeCun integrierte Gradientenlernen durch Backpropagation,
wodurch das gesamte Netzwerk erstmals auf ein gemeinsames Ziel zu trainieren konnte.
Neocognitron besaß zudem, im Gegensatz zu LeCuns CNN,
keine Gewichtsverteilung mithilfe von Filtern, wie es in heutigen CNN Modellen standard ist.
Zudem hatte Fukushima keine praktische Anwendung.
Er hatte die Idee und wie man diese umsetzt, doch durch damalige Verhältnisse war es für ihn schwierig diese umzusetzen.
Für AMT-Systeme wurde CNN jedoch erst ungefähr im Jahre 2015 relevant.
\cite{sigtia2016end}
In Siddharth Sigtia Arbeit transkribierte dieser
erstmals polyphone Musikstücke mithilfe von CNNs, und weiteren KI-Modulen.
Seitdem sind CNNs ein wichtiger bestandteil der modernen automatischen Musiktranskription.

Ein CNN gibt als letzte Ausgabe einen 3D-Tensor zurück,
welcher aus den Zeitfenster (Frames / T), der Frequenzachsenlänge (F) und der Anzahl der Filter (C) besteht.
\[
\text{CNN-Ausgabe} \in \mathbb{R}^{T \times F \times C}
\]
Meist schließt sich nach einem CNN, in einem AMT-System, ein RNN als Nächstes an.
Dieses kann jedoch nur 1D-Vektoren verarbeiten und nicht einen 3D-Tensor.
Deshalb wandeln wir vor der übergabe diesen 3D-Tensor um.
Jedes Zeitfenster aus dem 3D-Tensor wird einzeln zu einem 1D-Vektor umgewandelt.
Dabei wird der Vektor eine Dimension von F × C erhalten.
Diese Vektoren werden dann an das folgende RNN weitergeleitet.

\subsection{Recurrent Neural Networks}
RNNs sind neuronale Netze, welche entworfen wurden um Daten mit zeitlicher Struktur zu verarbeiten.
Dabei ist die besonderheit von RNNs das diese ein Gedächtnis haben.
Wenn nun in unserem Fall von AMT-Systemen eine bestimmte Tonabfolge gespielt wurde,
kann sich das RNN diese merken und dementsprechend die fortlaufenden Ausgaben anpassen.
Dies passiert dank den Hidden States.
Dieser stellt das Gedächtnis des RNNs dar und ist einer der wichtigsten Faktoren in einem RNN\@.
Mehr zu den Hidden States erläutere ich im übernächsten Paragraf.
Dieses Prinzip ist in der Musiktranskription sehr hilfreich,
da jede Note stark von den vorherigen gespielten Noten abhängt.
Takt, Rhythmus, Harmonie und die Melodie eines Musikstückes sind alles gute Beispiele,
warum diese sequenzielle Abfolge so passend in AMT-Systemen ist.

\subsubsection{Grundstruktur eines RNNs}
RNNs haben in der automatischen Musiktranskription mehrere Aufgabenfelder.
Je nachdem wie man das RNN trainiert bewältigt dieses alle Aufgaben oder nur einen Teil.
Diese Aufgaben bestehen aus Frame-Glättung (Temporal smoothing), Kontext-Modellierung (Temporal context modeling),
Feature-Zusammenführung (Sequential integration of acoustic features)
und Ausgabevorbereitung (time-distributed output classification).
Diese Aufgaben werden auf jedes Zeitfenster, auf jeden 1D-Vektor des CNNs, angewendet.
Doch bevor man sich auf diese einzelnen Aufgaben konzentrieren kann, muss man noch wissen, was Hidden States sind.

Hidden States sind das grundlegende Prinzip, warum RNNs funktioniere.
Sie stellen praktisch das Gedächtnis des RNNs dar und helfen somit
anderen Modulen Vorhersagen über bestimmte musikalische Eigenschaften zu treffen.
Für jeden 1D-Vektor, den wir vom CNN geliefert bekommen, wird ein Hidden State erstellt.
Diese werden sequenziell nacheinander, mit abhängigkeit des vorherigen Hidden States, definiert.
\[
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})
\]
In Hidden States ist das Wissen der vorherigen Zeitfenster über zeitliche Muster, wie Sustain und Akkordstruktur.
Sie alleine können jedoch noch keine eigene Entscheidung über bestimmte Noten fallen.
Dafür müssen andere Module die Informationen der Hidden States später richtig verwerten.
Jeder Hidden State hat eine Anzahl von Dimensionen, welche gleich der Anzahle der Neuronen in einem RNN ist.

\subsubsection{Aufgaben eines RNNs}
Bei der Frame-Glättung bekommt das RNN als Input die Aktivierungswerte des Zeitfensters.
Es betrachtet diese mit Kontext zu den vorherigen Zeitfenstern, um falsche vorhersagen auszuschließen.
Das CNN hat dem RNN schon Vorhersagen für bestimmte Noten gegeben.
Diese könnten jedoch fehlerhaft.
Zum Beispiel kann die Note C4 für den Frame 6 und 8 aktiv sein,
aber bei dem Frame 7 hat das CNN diese Note nicht als aktiv angesehen.
Es ist sehr unwahrscheinlich, das eine Note für nur einen Frame ausfällt.
Solche Arten von Fehlern verarbeitet das RNN und glättet dementsprechend die vorher vom CNN gelieferten Daten.
Dadurch ist der Ton C4 auch im Frame 7 aktiv.

Die Kontext-Modellierung ist etwas komplizierter als die Frame-Glättung.
Als Input bekommt diese auch die 1D-Vektoren des CNNs.
In der Kontext-Modellierung werden größere zeitliche Zusammenhänge betrachtet.
So kann die Kontext-Modellierung, mithilfe des Hidden States,
den Notenverlauf oder auch die länge einer Note vorhersehen.
Je nach den Trainingsdaten ist es zum Beispiel üblicher das auf C4 ein D3 folgt,
was durch die Kontext-Modellierung angepasst wird.
Dies ist aber immer stark von der Musikrichtung und Musikern abhängig, welche in den Trainingsdaten vorhanden sind.
Jazz und Pop oder Bach und Taylor Swift unterscheidet sich der style der Tonabfolge extremst.
Zudem wird Onset, Sustain und Offset stabilisiert.
Durch vorherige Beispiele weis die Kontext-Modellierung,
wie lange eine bestimmte Note andauern wird und kann so das Offset der Note einschätzen.
Als Output bekommt man Kontextabhängige Vektoren.
Sie haben die gleiche Struktur wie die 1D-Vektoren vom CNN, sind aber entsprechend dem Kontext angepasst.

Die Feature-Zusammenführung ist der letzte wichtige interne Schritt eines RNNs.
Bei diesem werden aus, lokalen alleinstehenden, Informationen eines Zeitfensters, konkrete musikalische Ereignisse.
Dadurch schreibt das RNN Ereignisse, wie Akkorde, Noten, Onsets und vieles mehr, heraus.
Dafür muss die Feature-Zusammenführung sich die einzelnen 1D-Vektoren als eine folge von Events anschauen.
Dies passiert wiedermal über den Hidden State.
Das wird vor allem wichtig, wenn man später eine MIDI-Datei ausgeben möchte,
da in dieser auch die einzelnen musikalischen Ereignisse aufgeschrieben sind.
Als Output bekommt man kontextreiche Vektoren.
Diese Vektoren sind, durch die vorherigen Module angepasste und verbesserte, Hidden States.

Die Ausgabevorbereitung ist der letzte Schritt des RNNs,
wodurch nun die gesammelten Daten zu echten musikalischen Ereignissen zusammengefügt werden.
Als Input werden die, durch die Feature-Zusammenführung verbesserten, Hidden States genutzt.
Zunächst werden diese durch ein Fully Connected Layer geschickt.
\[
\mathbf{z}_t = W \cdot \mathbf{h}_t + \mathbf{b}
\]
Ein Fully Connected Layer ist ein Klassifikator, welcher den Hidden States auf eine gewünschte Dimension bringt.
Wenn man zum Beispiel Klaviertasten vorhersagen möchte werden alle Hidden States auf mit 88 Dimensionen ausgestattet.
Bei MIDI-Dateien wären es 128 Dimensionen.
Die Werte, welche wir aus dem Fully Connected Layer herausbekommen sind, jedoch noch nicht richtig interpretierbar.
Um diese als konkrete und normalisierte Wahrscheinlichkeiten darstellen zu können,
nutzen wir noch eine Aktivierungsfunktion.
Zum Beispiel können wir mit der Sigmoid-Funktion als Aktivierungsfunktion,
alle Werte normiert in einem Bereich zwischen 0 und 1 bringen.
Sagen wir, wir wollen jetzt die gespielten Klaviertasten vorhersagen.
Dann hat jeder Hidden State jetzt für alle Klaviertasten einen eigenen Wert mit einer Wahrscheinlichkeit,
das diese Taste zu dem gewählten Moment gespielt wurde.
Zu guter Letzt müssen wir noch einen Threshold bestimmten.
In polyphonen Musikstücken können immer mehrere Noten gleichzeitig erklingen,
weshalb man nicht einfach die Note mit der höchsten Wahrscheinlichkeit auswählen kann.
Deshalb nutzt man einen Threshold, zum Beispiel bei 50\%,
welcher bestimmt wie viel Prozent eine Note braucht um als aktiv zu gelten.
Durch Postprocessing können einige Eigenschaften wie Rauschen noch herausgefiltert werden.
Postprocessing ist jedoch nicht relevant für den KI-Ablauf.
Wenn man zufrieden mit dem Ergebnis ist, kann man dieses jetzt in das gewünschte Output-Format,
meistens MIDI-Dateien, einfügen.

Heutzutage sind RNNs sind nur die grundlegende Struktur.
Basierend auf dieser Struktur gibt es einige bessere Systeme,
welche Aktiv in AMT-Systemen und anderen KI-Systemen genutzt werden.
Zwei dieser Systeme sind Long Short-Term Memory's (LSTM) und Bidirektionale RNNs (BiRNN).
Auf diese werde ich in den folgenden Kapiteln weiter eingehen.

\subsubsection{Long Short-Term Memory}
LSTMs sind verbesserte RNNs.
Diese kontrollieren durch Gates besser, welche Daten sie wirklich in den Hidden State speicher möchten.
Dadurch kann man das neuronale Netz noch weiter an seine gewünschten Ansprüche anpassen.
Ein normals RNN berechnet einen Hidden State mit folgender Formel:
\begin{equation}
h_t = \tanh(W x_t + U h_{t-1} + b)
\end{equation}
Dabei werden alle Daten, egal ob sinnvoll oder nicht, miteinander kombiniert.
Somit kann sich das RNN langfristig schwieriger Information merken.
Wenn zum Beispiel im 2\. Hidden State ein Onset erkannt wurde, kann der 20\. Hidden State
sich das schlechter merken, da ganz viele andere Informationen mitgeschrieben wurden.
LSTMs lösen dieses Problem mit Forget, Input und Output Gates und dem Cell State.
Der Cell State stellt das Langzeitgedächtnis des LSTMs dar.
Er berechnet sich aus den 3 Gates.
Das Forget Gate bestimmt, welche Informationen aus dem vorherigen Cell State gelöscht werden sollen.
Das Input Gate bestimmt, welche neuen Inhalte aus dem neuen Zeitfenster aufgenommen werden.
Dabei ist der Cell-candidate die Datenmenge, welche zum Speichern, durch das Input Gate, vorgeschlagen wird.
Das Output Gate bestimmt, welcher Teil des Cell States zu dem neuen Hidden State hinzugefügt wird.
\begin{align}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &&\text{(Forget Gate)} \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &&\text{(Input Gate)} \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &&\text{(Output Gate)} \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \quad &&\text{(Cell-candidate)} \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad &&\text{(Cellstate)} \\
h_t &= o_t \odot \tanh(c_t) \quad &&\text{(Aktueller Hidden State)}
\end{align}
Dadurch kann sich ein LSTM die verschiedenen musikalischen Ereignisse,
über das gesamte Audiosignal, besser im Zusammenhang merken.

\subsubsection{Bidirektionale RNNs}
Um ein noch besseres Ergebnis zu erhalten, kann man auch ein Bidirektionales RNN nutzen.
Dieses besteht aus zwei RNNs.
Eines liest die Zeitfenster von vorne und das andere von hinten ab.
Dadurch bekommt man die doppelte Menge Hidden States und die gesamte Vorhersage wird sehr viel robuster.
\[
\mathbf{h}_t = \left[ \overrightarrow{\mathbf{h}}_t \ ;\ \overleftarrow{\mathbf{h}}_t \right]
\]
Auch in AMT-Systemen sind Bidirektionale RNNs sehr hilfreich,
da musikalische Ereignisse auch eine klar verständliche Abhängigkeit, von der Zukunft in die Vergangenheit aus, haben.
Das gleiche Prinzip kann man auch auf LSTMs einsetzen.
Bidirektionale LSTMs sind noch robuster, aber dafür ist der Rechenaufwand bei weitem höher.
Ein Bidirektionales RNN kann man auch in der Arbeit von Curtis Hawthorne finden.
\cite{hawthorne2017onsets}

\subsubsection{Die Geschichte von RNNs}
Die Geschichte von Recurrent neural networks reicht bis in das Jahr 1990 zurück.
Damals wurde das allererste RNN von Jeffrey L. Elman gebaut.
\cite{elman1990finding}
Er baute erstmals die Idee der Rückkopplung ein,
wodurch der Output eines Neurons als zusätzliche Eingabe im nächsten Zeitfenster genutzt wird.
Das resultierte später in den Hidden States.
Diese Arbeit baute den Grundstein für alle weiterführenden RNNs.
Eine weitere wichtige Arbeit über RNNs stammt von Michael I. Jordan.
\cite{jordan1997serial}
Er publizierte seine Arbeit über RNNs offiziell im Jahre 1997,
obwohl er schon 1986 einen technischen Bericht über seine Arbeit veröffentlichte.
Zudem wurde, im Jahre 1997, das erste Bidirektionale RNN, von Mike Schuster,
\cite{schuster1997bidirectional}
und das erste LSTM von Hochreiter und Schmidhuber erfunden.
\cite{hochreiter1997long}
RNNs fanden ihren Weg in die automatische Musiktranskription, fast zeitgleich zu CNNs, im Jahre 2015.
\cite{sigtia2015hybrid}
Für LSTMs dauerte die Einbindung in AMT-Systeme etwas länger.
Erst im Jahre 2016 hat Siddharth Sigtia ein LSTM in einem AMT-System eingefügt.
\cite{sigtia2016end}
Dahingegen wurde das erste Bidirektionale RNN erst Ende 2017 in ein AMT-System, von Hawthorne, integriert.
\cite{hawthorne2017onsets}

\subsection{Transformers}

\subsection{Weitere Deep-Learning-Module und Entwicklungen}

