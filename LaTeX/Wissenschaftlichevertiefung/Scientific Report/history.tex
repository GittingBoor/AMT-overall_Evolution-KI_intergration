\section{Geschichte}

\subsection{Moorer und eines der ersten Musiktranskriptionssysteme}
\subsubsection{Grundlegende Probleme bei AMTs}
Einer der ersten Papers über Automatic Music Transcription wurde von James A. Moorer im Jahr 1977 geschrieben.
\cite{Moorer1977}
In diesem beschreibt Moorer seinen Ansatz, polyphone Musik Audiospuren
direkt über Computerprogramme in Notenschrift zu übertragen.
Während dieses Prozesses fallen ihm schon sehr viele schwierigkeiten auf, die auch in späteren Papern und Arbeiten
eine ausschlaggebende Rolle spielen werden.

Eines dieser Probleme wird von ihm als das \("\)Cocktail-Party-Problem\("\) bezeichnet.
Dieses stellt die Schwierigkeit dar, auf einer Party bestimmten Stimmen zu folgen, während viele verschiedene Stimmen
gleichzeitig erklingen.
Das gleiche Problem liegt in der Noten transkription.
Die meisten Musikstücke haben mehrere Instrumente, welche gleichzeitig spielen.
Öfter gibt es auch Musikstücke wo es für ein Instrument, wie zum Beispiel Violine, mehrere verschiedene Stimmen gibt.
Dies erschwert die Zuordnung bestimmter Noten zu einer gewählten Stimme.
Schon viele Menschen scheitern deshalb daran große Musikstücke richtig zu transkribieren.
Noch schwieriger wird es hier für Computerprogramme.
Anfangs identifizieren diese bestimmte Töne anhand der Frequenz des Tons.
Leider reicht das, wie Moorer feststellt, nicht aus um genaustens zu bestimmen,
welche Töne genau momentan gespielt werden.

Jeder Ton hat Obertöne.
Diese Obertöne sind jeweils das Vielfache von dem Grundton, den man spielt.
Heißt, wenn ich auf einem Klavier den Ton C3 mit 130,81 Hz spiele dann hat dieser
die Obertöne C4 (261,62 Hz), G4 (392,42 Hz) usw.
Wenn man nur C3 spielt erklingen für das Computerprogramm auch die jeweiligen Obertöne,
was man Frequenzüberlagerung nennt.
Diese Frequenzüberlagerung sorgt dafür, das zum Beispiel ein Klavier anders klingt als eine Violine.
Daraus resultiert dann die Klangfarbe (Timbre) eines bestimmten Instrumentes.
\cite{goswami2013timbre}
Leider konnte Moorer zu dieser Zeit noch nicht die Klangfarbe eines Instruments erkennen.
Er konnte auch noch nicht das Problem der Obertöne lösen,
da die damaligen Algorithmen und Verfahren noch nicht in der Lage waren die Grundfrequenz von den Obertönen zu trennen,
weshalb er sich ausschließlich auf zweistimmige polyphone Musikstücke fokussierte.

Ein weiteres Problem war Rauschen in realistischen Audiospuren und
Stilistische mittel in der Musik, wie zum Beispiel Vibrato.
In real aufgenommenen Audiospuren gibt es immer ein gewisses Hintergrundrauschen.
\cite{iZotope2025noisefloor}
Dieses kann von einem Computerprogramm auch als Note erkannt werden oder verhindern, das bestimmte Noten
richtig vom Computerprogramm erkannt werden.
Moorer hat ein Musikstück analog aufgenommen und dieses dann mit einem 14-Bit converter digitalisiert.
Dadurch war das Rauschen nicht weg, aber da er das Musikstück selber aufgenommen hat und dieses konvertiert hat
sorgte es für insgesamt geringeres Rauschen.
Zudem konnte er so die Musiker davon abhalten bestimmte Stilistische mittel zu verwenden,
um bessere Daten zur Transkription zu erhalten.
Stilistische mittel, wie Vibrato, konnten nicht genutzt werden,
da diese eine kleine aber kontinuierliche Veränderung der Frequenz verursachen.
Dadurch kann das Computerprogramm nicht korrekt erkennen, das eigentlich eine einzelne Note gespielt wurde.
Somit war der Onset und Offset der Note komplett falsch.

Das letzte Problem, was Moorer angesprochen hat, ist das Nutzen von nicht
harmonischen Instrumenten wie Trommeln oder einem Schlagzeug.
Diese Instrumente haben keinen eindeutigen Pitch für deren Töne,
sie sind eher abhängig von Rhythms und Lautstärke.
Da Moorers AMT sich jedoch auf das Frequenzmuster der Noten fokussiert,
können diese Musikinstrumente nicht berücksichtigt werden.

\subsubsection{Der Aufbau von Moorers AMT-Systems}
Moorers automatische Musiktranskriptionssystem war eins der ersten seiner Art.
Viele weiteren AMT-System leiten sich von diesen ab.

Zunächst wird ein analoges Musiksignal mit einem 14-Bit converter digitalisiert.
Dieses digitale Musiksignal wurden dann genutzt um mithilfe von Bandpassfiltern,
ein Filter welcher nur bestimmte Frequenzen durchlässt,
bestimmte Frequenzbereiche zu isolieren.
Dadurch konnte Moorer die gespielte Note und deren Dauer,
also zugleich auch deren Onset und Offset, feststellen.

Nun mussten die bestimmten Noten einer gewählten Stimme zugeordnet werden.
Dies wurde durch melodische Gruppierung verwirklicht.
Zunächst wurden Inseln gebildet.
Inseln sind Noten die sich Zeitlich vollständig überlappen.
Wir gehen davon aus das jede Stimme nur eine Note gleichzeitig spielt,
wodurch diese Noten nicht der gleichen Stimme zugeordnet werden können.
Als Nächstes müssen die anderen Noten auf verschiedene Kombinationen getestet werden.
Desto kleiner die Frequenz sprünge je Note sind, desto wahrscheinlicher gehören sie einer Stimme zu.
Zudem werden Gruppierungen von Noten erstellt, welche am wahrscheinlichsten harmonisch nacheinander gespielt worden.

Zum Schluss ließ Moorer die gewonnenen Daten durch ein Programm laufen, 
um diese dann mithilfe eines Plotters in eine Notenschrift umzuwandeln.

\subsection{MIDI-Dateien}
Notenschrift als Input für ein Computerprogramm, Synthesizer oder ähnliches ist unhandlich.
Zunächst müsste man die gespielten Noten immer wieder zu Notenschrift konvertieren
und danach diese auch noch in anderen Programmen analysieren, was sehr aufwändig werden würde.
Eine bessere lösung dafür wäre eine Datenschreibweise,
bei der alle wichtigen Informationen bestimmter Noten übersichtlich aufgeschrieben sind.
MIDI-Dateien sind dafür perfekt geeignet.

MIDI ist ein Standartprotokoll zur kommunikation zwischen
elektronischen Musikinstrumenten, Computern und anderen Geräten wie zum Beispiel Synthesizer.
Dieses Protokoll wurde 1983 erstmals eingeführt und wurde schnell zu einem Standard in der digitalen Musikindustrie.
\cite{smith1983midi}
In MIDI-Dateien werden Daten von Tönen gelagert,
welche zum Beispiel zuvor von einem elektronischen Instrument gespielt wurden oder durch AMTs erfasst wurden.

In MIDI-Dateien werden folgenden Daten gespeichert:
\begin{enumerate}
    \item \textbf{MIDI Header-Chunk (MThd):} Enthält grundlegende Informationen zur Struktur der Datei:
    \begin{itemize}
        \item Formattyp (0 = eine Spur, 1 = mehrere synchrone Spuren, 2 = unabhängige Spuren)
        \item Anzahl der folgenden MTrk-Blöcke (Tracks)
        \item Zeitauflösung (Ticks pro Viertelnote)
    \end{itemize}

    \item \textbf{MIDI Track-Chunks (MTrk):} Jede Spur enthält eine zeitlich sortierte Liste von MIDI-Events:
    \begin{itemize}
        \item \textbf{MIDI-Events:}
        \begin{itemize}
            \item Note Onset / Offset
            \item Control Change (Lautstärke)
            \item Program Change (gibt das spielende Instrument an)
            \item Pitch Bend (verändert die Tonhöhe)
            \item Aftertouch / Polyphonic Key Pressure (Druckstärke pro Taste)
        \end{itemize}
        \item \textbf{Meta-Events:}
        \begin{itemize}
            \item Set Tempo (Tempo in Mikrosekunden pro Viertelnote)
            \item Time Signature (Taktart zum Beispiel 4/4 oder 3/4)
            \item Key Signature (Tonart zum Beispiel C-Dur oder A-Moll)
            \item Track Name
            \item Lyrics
            \item Markers
            \item End of Track (Ende einer Spur)
        \end{itemize}
        \item \textbf{System Exclusive Events (SysEx):}
        \begin{itemize}
            \item Herstellerspezifische Daten wie Synthesizer-Presets oder Spezialbefehle
        \end{itemize}
    \end{itemize}

    \item \textbf{Delta-Time:} Gibt die Zeit (in Ticks) an, die seit dem letzten Event vergangen ist:
    \begin{itemize}
        \item Ermöglicht die genaue zeitliche Positionierung jedes MIDI-Events
        \item Grundlage für das Timing und die rhythmische Struktur der Datei
    \end{itemize}
\end{enumerate}
Am wichtigsten sind dabei die MTrk-Blöcke, in denen die Daten der einzelnen Noten gespeichert werden.
Dabei stellt ein Track die Eventliste einer ganzen Stimme dar,
wie zum Beispiel die Melodiestimme, eine Violine, die Pedalsteuerung eines Klaviers oder Metadaten.
Es fällt auf das diese vier Beispiele alle sehr unterschiedliche Aufgaben und bedeutungen haben.
Das liegt daran, dass in MIDI-Dateien eher zusammenhängende Funktionen gespeichert werden und nicht nur Musiknoten.

MIDI-Dateien kamen auch der Forschung für AMT-Systemen sehr gelegen,
da man nun ein standardisiertes output Format für diese Programme besaß.
Später werden diese zudem sehr Essenziell bei dem Training KI basierter AMT-Systeme.
\cite{telila2025cnn}

\subsection{Polyphone AMT-Systeme und neue Ansätze}
\subsubsection{Martins Blackboard-System}
Moorer stellte, mit seinem AMT-System, viele grundlegende Bausteine für dieses Forschungsgebiet.
Jedoch gab es noch viele offene Probleme, die noch überwunden werden mussten.
Eines der ausschlaggebendsten Probleme stellte sich Keith D. Martin.
\cite{Martin1996}
Zuvor wurden höchstens zwei verschiedene Stimmen gleichzeitig zur Musik transkription verwendet.
Ein großer Teil von Musikstücken verwendet jedoch mehr Stimmen.
Um diese polyphonen Musikstücke zu transkribieren,
baute Martin einen neuen Ansatz eines AMT-Systems mit innovativen Modulen und Ansätzen.

Nachdem das Input Audiosignal in ein Correlogram verarbeitet wurde,
spaltet sich Martins AMT-System auf, in einen Analysepfad und einen Rhythmuspfad.
Dabei konzentriert sich der Analysepfad auf die Zusammenhänge der verschiedenen Noten,
während der Rhythmuspfad sich mehr mit der Lautstärke und den Notenanschlägen beschäftigt.
Am Ende werden diese Kenntnisse zusammengeführt.
Der Output besteht aus dem Onset, der Dauer und der Frequenz jeder gespielten Note.
Martin gibt nicht explizit zurück, welcher Stimme jede Note zugeordnet ist.
Durch das Blackboard-System, und vor allem nach sequentieller Analyse der Intervalle, kann man
jedoch die erhaltenen Noten später bestimmten Stimmen nachträglich korrekt zuordnen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Graphics/Martin1996Structure}
    \caption{Systemstruktur des AMT-Ansatzes von Keith D. Martin (1996), basierend auf \cite{Martin1996}.}
    \label{fig:martin-structure}
\end{figure}

Martin nutzte ein Blackboard-System zur Notenerkennung und Hypothesenbildung.
Unter anderem kann man dadurch Noten bestimmten Stimmen zuordnen.
Ein Blackboard-System ist ein kollaboratives Problemlösungsmodell, welches aus verschiedenen Modulen besteht,
die alle auf einem gemeinsamen Datenraum Hypothesen aufstellen können.
Um diese Vermutungen aufstellen zu können müssen jedoch erst die dafür nötigen Daten erarbeitet werden.
Dies erreicht Martin mithilfe von Sinuskurvenanalyse.
Zur Sinuskurvenanalyse, in Martins Aufbau, gehören die Bestandteile
Correlogram, Envelope, Summary Autocorrelation und Peaks.
Durch diese können Eigenschaften der Noten ermittelt werden, auf denen aufbauend später Hypothesen entstehen.
Die Module des Blackboard-Systems sind dementsprechend
Onsets, Periodicities und Simultaneous, Sequential Intervals.
Diese Module geben sich gegenseitig mehr Informationen über die gespielten Noten,
sodass später gegebene Stimmen unterschieden werden können.

Der Analysepfad beginnt bei dem Correlogram.
Hier wird der Input, ein reales Audiosignal,
zunächst verarbeitet und dann in einem Correlogram dargestellt.
Dieses Visualisiert die Korrelation der gegebenen Noten in Abhängigkeit der Zeit, Frequenz und Lag.
Lag soll in diesem Fall darstellen, wann sich ein bestimmtes Signal wiederholt.
Dabei ist die Wiederholung ein Signal, welches dem Ursprungssignal maximal selbst ähnelt.
Heißt, wenn ich einen Ton mit der Frequenz x spiele
und dieser sich nach zum Beispiel 10ms wiederholt wird das rechnerisch durch den Lag dargestellt.
Als Nächstes wird mithilfe der Summary Autocorrelation das Correlogram komprimiert
und die Datenstruktur normalisiert.
Dadurch entsteht eine stabile Grundfrequenz,
sodass die weiteren schritte effizient das Correlogram untersuchen können.
Nun werden basierend auf der normierten Struktur die Peaks gesucht.
Diese Peaks deuten darauf hin, wann periodische Komponenten auftreten.
Dabei geben sie nicht den Onset der Noten zurück, sondern nur die generelle Frequenz zu einer bestimmten Zeit.
Darauf werden mehrere Peaks, die regelmäßig wiederkehren, gruppiert.
So kann man Muster in der gespielten Musik erkennen und kurzzeitige Störfaktoren ausschließen.

Im Rhythmuspfad wird wiederum die zeitliche Analyse der Noten durchgeführt.
Zunächst wird mit dem Envelope die Lautstärke des Signals zu jedem Zeitpunkt festgelegt.
Dies dient als Grundlage, um anschließend die Onsets der Noten zu bestimmen.
Jeder Onset wird sehr präzise im Envelope dargestellt durch einen plötzlichen Anstieg der Lautstärke.

Nachdem die beiden Pfade durchgelaufen sind, hat man schon Noten,
welche man grundsätzlich in Notenschrift transkribieren kann.
Jedoch sind diese Noten musikalisch noch nicht verstanden worden.
In der oben gegebenen Abbildung gibt es noch die Bestandteile Simultaneous und Sequential Intervals.
In dem Modul Simultaneous Intervals wird ermittelt, welche Töne gleichzeitig erklingen.
Diese können, wenn sie harmonisch zueinander sind, zu einem Chord gebunden werden.
Sequential Intervals analysieren dahingegen, welche Töne nacheinander gespielt werden und
möglicherweise eine Melodie bilden könnten.
Dies erfolgt durch Tonhöhenverläufe, Pausen und Frequenzunterschiede.
Dadurch können Melodien, Basslinien oder Begleitungen voneinander getrennt werden.
Zum Schluss werden dadurch die verschiedenen Stimmen, in Martins Modell, aufgetrennt.

\subsubsection{Kalpuros xxx-System}
AAAA