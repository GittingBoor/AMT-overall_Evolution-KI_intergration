\section{Geschichtliche Entwicklung von AMT-Systemen}

\subsection{Moorer und das erste AMT-System}
Eine der ersten Arbeiten über automatische Musiktranskription wurde im Jahr 1977 geschrieben\cite{Moorer1977}.
In dieser beschreibt Moorer seinen Ansatz, polyphone Musikaudiospuren
direkt über Computerprogramme in Notenschrift zu übertragen.
Während dieses Prozesses fallen ihm schon große Probleme auf, die auch in späteren Arbeiten
eine ausschlaggebende Rolle spielen.

\subsubsection{Grundlegende Probleme}
Eines dieser Probleme wird von ihm als das \enquote{Cocktail-Party-Problem} bezeichnet.
Dieses stellt die Schwierigkeit dar, auf einer Party bestimmten Stimmen zu folgen, während viele verschiedene Stimmen
gleichzeitig erklingen.
Das gleiche Problem existiert in der Noten transkription.
Die meisten Musikstücke haben mehrere Instrumente, welche gleichzeitig spielen.
Oft gibt es auch Musikstücke, bei denen es mehrere Stimmen für ein Instrument gibt.
Ein Beispiel dafür ist die Violine, die zum Beispiel in Orchesterstücken oft auf mehrere Stimmen (erste, zweite, dritte) aufgeteilt wird.
Dies erschwert die Zuordnung bestimmter Noten zu einer gewählten Stimme.
Der Großteil aller Menschen scheitert daran große Musikstücke richtig zu transkribieren.
Noch schwieriger wird es für Computerprogramme.
Anfangs identifizieren diese bestimmte Töne anhand der Frequenz des Tons.
Moorer stellte fest, dass sich allein anhand der Frequenz nicht eindeutig bestimmen lässt,
welche Töne zu einem bestimmten Zeitpunkt gespielt werden.

Das nächste Problem ist die Frequenzüberladung durch Obertöne.
Jeder Ton hat Obertöne.
Diese Obertöne sind jeweils das Vielfache von dem Grundton, den man spielt.
Spielt man beispielsweise auf einem Klavier
den Ton C3 (130,81 Hz), so entstehen die Obertöne C4 (261,62 Hz), G4 (392,42 Hz) und weitere.
Wenn man nur C3 spielt, erklingen für das Computerprogramm auch die jeweiligen Obertöne,
was man Frequenzüberlagerung nennt.
Diese Frequenzüberlagerung sorgt dafür, das zum Beispiel ein Klavier anders klingt als eine Violine.
Daraus resultiert die Klangfarbe (Timbre) eines bestimmten Instrumentes\cite{goswami2013timbre}.
Moorer konnte das Problem der Obertöne nicht lösen.
Zum einen war es ihm aufgrund des damaligen technischen Stands nicht möglich, die Klangfarbe eines Instruments zu erkennen.
Zum anderen waren die Algorithmen und Verfahren jener Zeit nicht in der Lage, die Grundfrequenz von den Obertönen zu trennen.
Deshalb fokussierte er sich ausschließlich auf zweistimmige polyphone Musikstücke.

Ein weiteres Problem war Rauschen in unter realen Aufnahmebedingungen erstellten Audiospuren
sowie stilistische Mittel in der Musik, wie etwa Vibrato.
In real aufgenommenen Audiospuren gibt es immer ein gewisses Hintergrundrauschen\cite{iZotope2025noisefloor}.
Dieses kann von einem Computerprogramm auch als Note erkannt werden oder verhindern,
dass bestimmte Noten richtig erkannt werden.
Moorer nahm das Musikstück selbst analog auf und digitalisierte es anschließend mit einem 14-Bit-A/D-Wandler.
Durch die eigene Aufnahme konnte er die Aufnahmeumgebung, die Mikrofonposition
und die genutzte Technik kontrollieren, wodurch sich das Rauschen im Vergleich zu unkontrollierten Aufnahmen verringerte.
Zudem wies er die Musiker an auf stilistische Mittel zu verzichten.
Stilistische Mittel wie Vibrato verursachen kleine, aber kontinuierliche Veränderungen der Frequenz.
Dadurch kann das Computerprogramm nicht korrekt erkennen, das eigentlich eine einzelne Note gespielt wurde.
Die Folge davon wäre eine falsche Zuordnung des Onsets und Offsets der Noten.
Ohne diese erhielt Moorer klarere und besser verwertbare Daten für die Transkription.

Das letzte Problem, was Moorer angesprochen hat, ist die Nutzung von nicht
harmonischen Instrumenten wie Trommeln oder einem Schlagzeug.
Diese Instrumente haben keinen eindeutigen Pitch.
Sie sind abhängig von dem Rhythms und der Lautstärke.
Da Moorers AMT sich jedoch auf das Frequenzmuster der Noten fokussiert,
konnten diese Musikinstrumente nicht berücksichtigt werden.

\subsubsection{Der Aufbau von Moorers AMT-System}
Moorers automatische Musiktranskriptionssystem war eins der ersten seiner Art.
Viele weitere AMT-Systeme leiten sich von diesem ab.

Zunächst wird ein analoges Musiksignal mit einem 14-Bit-A/D-Wandler digitalisiert.
Dieses digitale Musiksignal wurde anschließend genutzt, um mithilfe von Bandpassfiltern,
bestimmte Frequenzbereiche zu isolieren.
Ein Bandpassfiltern ist ein Filter welcher nur bestimmte Frequenzen durchlässt.
Dadurch konnte Moorer die gespielte Note und deren Dauer,
wie deren Onset und Offset, feststellen.

Nun mussten die bestimmten Noten einer gewählten Stimme zugeordnet werden.
Dies wurde durch melodische Gruppierung verwirklicht.
Zunächst wurden Inseln gebildet.
Inseln sind Noten die sich Zeitlich vollständig überlappen.
Wir gehen davon aus das jede Stimme nur eine Note gleichzeitig spielt,
wodurch diese Noten nicht der gleichen Stimme zugeordnet werden können.
Als Nächstes müssen die anderen Noten auf verschiedene Kombinationen getestet werden.
Je geringer die Frequenzunterschiede zwischen aufeinanderfolgenden Noten sind,
desto wahrscheinlicher stammen sie aus derselben Stimme.
Dies liegt daran, dass melodische Linien in der Regel durch kleine, schrittweise Intervalle fortgeführt werden,
während größere Sprünge häufiger auf einen Wechsel der Stimme hindeuten.
Zudem werden Gruppierungen von Noten erstellt, welche am wahrscheinlichsten harmonisch nacheinander gespielt worden.

Zum Schluss ließ Moorer die gewonnenen Daten durch ein Programm laufen, 
um diese dann mithilfe eines Plotters in eine Notenschrift umzuwandeln.

\subsection{Hidden Markov Modelle}\label{subsec:hiddenMarkov}
Hidden Markov Modelle (HMM) sind statische Modelle, welche sich sehr gut zur Analyse von Musikstücken eignen.
Sie wurden erstmals in den 1960er Jahren erfunden\cite{baum1970maximization} und
sind ein zentraler bestandteil früher AMT-Systeme.
HMMs beschreiben eine Abfolge von \enquote{versteckten Zuständen},
welche im Kontext von AMT-Systemen Noten im Audiosignal darstellen.
Durch indirekt beobachtbare Daten, wie zum Beispiel die Spektraldaten des Audiosignals,
können diese Noten erschlossen werden.

HMMs bestehen aus:
\begin{itemize}
    \item Zustände (States)
    \item Übergangswahrscheinlichkeiten (Transition Probabilities)
    \item Emissionswahrscheinlichkeiten (Emission Probabilities)
    \item Beobachtungen (Observations)
\end{itemize}
Nehmen wir das Beispiel eines Klaviers.
Ein Klavier hat 88 Tasten, und somit mindestens 88 Zustände.
Durch Akkorde können zudem mehr Zustände generiert werden.
Die Übergangswahrscheinlichkeit stellt dar,
wie wahrscheinlich es ist, von einem Zustand zu einem bestimmten anderen Zustand zu wechseln.
Zum Beispiel könnte es wahrscheinlicher sein, dass auf die Note C4 der Ton G3 folgt statt D1,
da diese Tonfolge harmonischer und musikalisch plausibler klingt.
Dies ist jedoch nur eine Annahme anhand von gesammelten Testdaten, weshalb es nicht als Begründung ausreicht.
Deshalb kommt als zweite Instanz die Emissionswahrscheinlichkeit hinzu.
Diese gibt an, wie wahrscheinlich ein bestimmter Zustand in der momentanen Beobachtung ist.
Die Beobachtung wird dabei zusammengesetzt aus Eigenschaften, wie Frequenzverteilung, Spektrogramm
oder anderen Merkmalen die man aus anderen Modulen herleiten kann.
Aus den gesammelten Daten kann nun mithilfe von zum Beispiel dem Viterbi-Algorithmus\cite{takeda2002hidden} die
wahrscheinlichste Abfolge von Zuständen berechnet werden.

In gewisser Weise lassen sich HMMs auf KI-Modelle übertragen.
Beide arbeiten mit versteckten Zuständen und
berechnen anhand von Testdaten die wahrscheinlichste Kombination von Zuständen.
Natürlich sind moderne KI-Modelle weitaus komplexer,
doch HMMs spielten im Kontext automatischer Musiktranskriptionssysteme eine zentrale Rolle dabei,
wie KI-Methoden in diesem Bereich später eingesetzt wurden.
Sie beeinflussten insbesondere den Umgang mit zeitlichen Abfolgen
und Unsicherheiten in der Tonerkennung,
was bis heute relevante Konzepte in KI-basierten AMT-Systemen sind.

\subsection{AMT-Datensätzen}
Während der Forschung an neuen AMT-Systemen wurden immer wieder neue Datensätze genutzt,
um das AMT-System zu bespielen oder heutzutage auch zu trainieren.
In der Transkription von Musikstücken sind diese sehr relevant,
da man nicht so leicht an nutzvolle Datensätze generieren kann und
mit dem weiteren Einsatz von KI-Modellen genau diese sehr wichtig, in größeren Mengen, sind.
Die ersten Datensätze, welche genutzt worden waren selbst kreierte Datensätze,
welche lediglich einige Eigenschaften der Noten beinhalten.
In den AMT-Systemen von Moorer\cite{Moorer1977} oder auch Martin\cite{Martin1996} wurde solch ein Ansatz verfolgt.
Durch die einführung von MIDI-Dateien wurde ein standard entwickelt,
welcher jetzt in dem großteil, der AMT-Systeme eingebaut werden sollte.
Auf der Grundlage von MIDI-Dateien wurden auch spezialisierte Datensätze erstellte.
Zwei der größten und wichtigsten sind MAPS (MIDI Aligned Piano Sounds) und
MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization).
Dabei ist MAPS ein Datensatz aus künstlich generierten Audiospuren,
während MAESTRO ein ungefähr 200 Stunden langer Datensatz von realen isolierten Studioaufnahmen ist.
Vor allem bei dem training neuer KI-Modellen sind solche großen Datensätze sehr hilfreich.
Ein weiterer wichtiger Datensatz ist ADTOF (Annotated Drum Transcription Onset Features).
Dieser besteht ausschließlich aus unharmonischen Instrumenten.
In Moorers und anderen AMT-Systemen ist die Erkennung von unharmonischen Instrumenten ein großes Problem.
KI-Modelle können mithilfe von dem ADTOF Datensatz sich speziell auf diese Instrumente vorbereiten.
Es gibt auch Methoden schnell seine eigenen Datensätze zu generieren.
Eine der neusten Methoden nutzt HMM- und Viterbi-basierte Alignment-Verfahren,
um synthetische Audiosignale zu kreieren, damit man mehr Daten hat um sein KI-Modell zu trainieren\cite{joysingh2019development}.

\subsection{Polyphone AMT-Systeme}
\subsubsection{Blackboard-System}
Moorer stellte, mit seinem AMT-System, grundlegende Bausteine für dieses Forschungsgebiet.
Jedoch gab es noch zahlreiche offene Probleme, die noch überwunden werden mussten.
Eines der ausschlaggebendsten Probleme war die polyphonie von Musikstücken\cite{Martin1996}.
Zuvor wurden höchstens zwei verschiedene Stimmen gleichzeitig zur Musik transkription verwendet.
Ein großer Teil von Musikstücken verwendet jedoch mehr Stimmen.
Um diese polyphonen Musikstücke zu transkribieren,
baute Martin einen neuen Ansatz eines AMT-Systems mit innovativen Modulen und Ansätzen.

Nachdem das Input Audiosignal in ein Correlogram verarbeitet wurde,
spaltet sich Martins AMT-System auf, in einen Analysepfad und einen Rhythmuspfad.
Dabei konzentriert sich der Analysepfad auf die Zusammenhänge der verschiedenen Noten,
während der Rhythmuspfad sich mehr mit der Lautstärke und den Notenanschlägen beschäftigt.
Am Ende werden diese Kenntnisse zusammengeführt.
Der Output besteht aus dem Onset, der Dauer und der Frequenz jeder gespielten Note.
Martin gibt nicht explizit zurück, welcher Stimme jede Note zugeordnet ist.
Durch das Blackboard-System, und vor allem nach sequentieller Analyse der Intervalle, kann man
jedoch die erhaltenen Noten später bestimmten Stimmen nachträglich korrekt zuordnen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Graphics/Martin1996Structure}
    \caption[Systemstruktur nach Martin]{Systemstruktur des AMT-Ansatzes von Keith D. Martin (1996), basierend auf \cite{Martin1996}.}
    \label{fig:martin-structure}
\end{figure}

Martin nutzte ein Blackboard-System zur Notenerkennung und Hypothesenbildung.
Unter anderem kann man dadurch Noten bestimmten Stimmen zuordnen.
Ein Blackboard-System ist ein kollaboratives Problemlösungsmodell, welches aus verschiedenen Modulen besteht,
die alle auf einem gemeinsamen Datenraum Hypothesen aufstellen können.
Um diese Vermutungen aufstellen zu können müssen jedoch erst die dafür nötigen Daten erarbeitet werden.
Dies erreicht Martin mithilfe von Sinuskurvenanalyse.
Zur Sinuskurvenanalyse, in Martins Aufbau, gehören die Bestandteile
Correlogram, Envelope, Summary Autocorrelation und Peaks.
Durch diese können Eigenschaften der Noten ermittelt werden, auf denen aufbauend später Hypothesen entstehen.
Die Module des Blackboard-Systems sind dementsprechend
Onsets, Periodicities und Simultaneous, Sequential Intervals.
Diese Module geben sich gegenseitig mehr Informationen über die gespielten Noten,
sodass später gegebene Stimmen unterschieden werden können.

Der Analysepfad beginnt bei dem Correlogram.
Hier wird der Input, ein reales Audiosignal,
zunächst verarbeitet und dann in einem Correlogram dargestellt.
Dieses Visualisiert die Korrelation der gegebenen Noten in Abhängigkeit der Zeit, Frequenz und Lag.
Lag soll in diesem Fall darstellen, wann sich ein bestimmtes Signal wiederholt.
Dabei ist die Wiederholung ein Signal, welches dem Ursprungssignal maximal selbst ähnelt.
Heißt, wenn ein Ton mit der Frequenz x gespielt wird
und dieser sich nach zum Beispiel 10ms wiederholt wird das rechnerisch durch den Lag dargestellt.
Als Nächstes wird mithilfe der Summary Autocorrelation das Correlogram komprimiert
und die Datenstruktur normalisiert.
Dadurch entsteht eine stabile Grundfrequenz,
sodass die weiteren schritte effizient das Correlogram untersuchen können.
Nun werden basierend auf der normierten Struktur die Peaks gesucht.
Diese Peaks deuten darauf hin, wann periodische Komponenten auftreten.
Dabei geben sie nicht den Onset der Noten zurück, sondern nur die generelle Frequenz zu einer bestimmten Zeit.
Darauf werden mehrere Peaks, die regelmäßig wiederkehren, gruppiert.
So kann man Muster in der gespielten Musik erkennen und kurzzeitige Störfaktoren ausschließen.

Im Rhythmuspfad wird wiederum die zeitliche Analyse der Noten durchgeführt.
Zunächst wird mit dem Envelope die Lautstärke des Signals zu jedem Zeitpunkt festgelegt.
Dies dient als Grundlage, um anschließend die Onsets der Noten zu bestimmen.
Jeder Onset wird sehr präzise im Envelope dargestellt durch einen plötzlichen Anstieg der Lautstärke.

Nachdem die beiden Pfade durchgelaufen sind, hat man schon Noten,
welche man grundsätzlich in Notenschrift transkribieren kann.
Jedoch sind diese Noten musikalisch noch nicht verstanden worden.
In der Abbildung-\titleref{fig:martin-structure} gibt es noch die Bestandteile Simultaneous und Sequential Intervals.
In dem Modul Simultaneous Intervals wird ermittelt, welche Töne gleichzeitig erklingen.
Diese können, wenn sie harmonisch zueinander sind, zu einem Chord gebunden werden.
Sequential Intervals analysieren dahingegen, welche Töne nacheinander gespielt werden und
möglicherweise eine Melodie bilden könnten.
Dies erfolgt durch Tonhöhenverläufe, Pausen und Frequenzunterschiede.
Dadurch können Melodien, Basslinien oder Begleitungen voneinander getrennt werden.
Zum Schluss werden dadurch die verschiedenen Stimmen, in Martins Modell, aufgetrennt.

\subsubsection{RASTA-basiertes System}
Ende 1997 publizierte Klapuri seine Masterarbeit \enquote{Automatic transcription of Music}\cite{klapuri1998automatic}.
In seiner Arbeit benutzt er ein neues RASTA-basiertes Verfahren, zur Unterdrückung nicht harmonischer Signalkomponenten.
Somit konnten auch Musikstücke mit nicht harmonischen Instrumenten, wie Trommeln, transkribiert werden.
Zudem führte er ein Modul ein, zur Schätzung der Anzahl an gleichzeitigen Stimmen, die in dem Musikstück vorkommen.
Klapuris AMT-System konzentriert sich, gegenüber Martins AMT-System, viel mehr auf die robustheit gegenüber
echten polyphonen Audioaufnahmen mit rauschen, Störgeräuschen und nicht harmonischen Instrumenten.
Die Struktur des Systems ist linear aufgebaut und sehr präzise in dem Aufgabenfeld, für die es entwickelt wurde.

\begin{figure}
    \vspace{1em}
    \begin{tikzpicture}[>=stealth, thick]

        % Obere Zeile
        \node at (0,0) (input) {Audio Input};
        \node at (5,0) (prep) {Vorverarbeitung};
        \node at (9,0) (onset) {Onset-Detektion};
        \node at (12.75,0) (rasta) {RASTA-Filtering};

        % Untere Zeile
        \node at (0.75,-1) (multi) {Multipitch-Schätzung};
        \node at (5,-1) (voice) {Stimmenanzahl};
        \node at (9,-1) (notes) {Note Formation};
        \node at (13,-1) (output) {Transkription};

        % Pfeile obere Zeile
        \draw[->] (input) -- (prep);
        \draw[->] (prep) -- (onset);
        \draw[->] (onset) -- (rasta);

        % Übergangspfeil
        \draw[->] (rasta.south) -- ++(0,-0.1) -| (multi);

        % Pfeile untere Zeile
        \draw[->] (multi) -- (voice);
        \draw[->] (voice) -- (notes);
        \draw[->] (notes) -- (output);

    \end{tikzpicture}
    \vspace{1em}
    
    \label{fig:klapuri_rasta}
    \caption[Struktur eines RASTA-basiertem AMT-System]{Eigene Darstellung: Struktur von Klapuris RASTA-basiertem AMT-System}
\end{figure}

Klapuris System ist sequentiell aufgebaut und besteht aus sechs Modulen.
Als Input wird eine reale Audioaufnahme genutzt, welche in ein Monosignal umgewandelt wird.
Dieses Monosignal wird mithilfe von STFT transformiert,
sodass durch das entstandene Spektrogramm Zeit,
Frequenz und Amplitude der gegebenen Noten ausgelesen werden kann.
Anhand dieser Daten werden als Nächstes die Onsets der Noten bestimmt.
Klapuri nutzt, zur Onset erkennung, ein Schema von Eric D. Scheirer\cite{scheirer1998tempo}.
Bei diesem wird das Audiosignal in Frequenzbänder aufgeteilt und
auf jedem Band die zeitliche Änderung der Energie analysiert.
Daraufhin werden die resultierenden Energie-Deviate der verschiedenen Bänder summiert.
Die nun entstehenden Peaks werden als Onsets der Noten interpretiert.
Als Nächstes werden unharmonische Instrumente und transiente Geräusche, wie zum Beispiel Rauschen, entfernt.
Dies erfolgt mithilfe der RASTA-Filterung, welches auf das spektrale Signal, vom gegeben Spektrogramm, wirkt.
Mehr zu der RASTA-Filterung wird im Abschnitt-\titleref{itm:rasta}, im Detail, erläutert.
Um mehrere Töne, die gleichzeitig im Signal erklingen, zu erkennen wird harmonic matching genutzt.
Die Multipitch-Schätzung funktioniert so, dass zu einer bestimmten Zeit immer die dominanteste Tonhöhe,
basierend auf der Energieverteilung im Signal, gesucht wird.
Sobald man diese ermittelt hat, wird sie vom Signal subtrahiert und dieses Verfahren wird erneut eingesetzt,
bis man jeden signifikanten Pitch untersucht hat.
Jetzt werden die Stimmenanzahlen geschätzt.
Dieses Modul wird im Abschnitt-\titleref{itm:multi} detailliert wiedergeben.
In dem Modul Note-Formation werden jetzt die gegebenen Daten zusammengeführt und in eine MIDI-Datei zusammengefasst.
Diese MIDI-Datei wäre jetzt bereit in Notenschrift transkribiert zu werden.

\begin{description}[style=nextline]
\item[RASTA-Filterung]\label{itm:rasta}
Die RASTA-Filterung ist eine der neuen Module von Klapuri.
Sie bewirkt, das nicht-harmonische Störsignale, wie
Rauschen, unharmonische Instrumente und vom Musikstück unabhängige Geräusche ausgefiltert werden.
Dadurch werden die harmonischen Komponenten des Stückes mehr herausgehoben,
wodurch folgende Module einfacher weitere Eigenschaften den gegebenen Noten zuordnen können.
Zudem stärkt dies die Robustheit der Multipitch-Schätzung,
in der mehrere Töne zur gleichen Zeit im Audiosignal erkannt werden müssen.
Dieses Verhalten wird erzielt, indem ein Filter gesetzt wird,
der schaut, wie Laut jede Frequenz zu jedem Moment ist.
Nach der Berechnung gibt der Filter eine Lautstärke-Kurve zurück.
Alle Frequenzanteile, die entweder zu kurzzeitig sind, wie etwa Claps oder Hi-Hats, oder zu langanhaltend,
wie dauerhaftes Rauschen, werden durch einen Bandpassfilter aus dem Audiosignal herausgefiltert.
Dadurch bleiben vor allem die zeitlich stabilen, musikalisch relevanten Komponenten erhalten.
\end{description}

\begin{description}[style=nextline]
\item[Multipitch-Schätzung]\label{itm:multi}
Auch die Stimmanzahl erkennung ist, von Klapuri, ein neu eingefügtes Modul.
Vor allem bei der Multipitch-Schätzung ist dieses sehr wichtig, da das System dadurch ein Bild davon bekommt,
wie viele Töne gleichzeitig erklingen können, zu einer bestimmten Zeit.
Die Multipitch-Schätzung zieht den spektralen Abdruck eines Tons von dem Audiosignal ab, wenn dieser erkannt wurde.
Jedoch kann diese nicht einschätzen, wann genau sie aufhören soll die Töne zu erkennen.
Da hilft die Stimmanzahlschätzung.
Diese analysiert das neue Audiosignal nach jeder Multipitch-Schätzung und gibt aus,
ob in der spektralen Energie noch Noten zu erkennen sind.
Dies ordnet die Stimmanzahlschätzung durch drei Eigenschaften ein.
Gibt es in der spektralen Energie noch typische Muster von harmonischen Klängen?
Wie viele Stimmen wurden schon erkannt?
Tragen die weiteren extrahierten Stimmen noch groß etwas zur Erklärung des gesamten Musikstückes bei?
Dadurch extrahiert die Multipitch-Schätzung nur die nötigen Noten und unnötig Störfaktoren werden ausgelassen.
Zudem kann man eine ungefähre Einschätzung bekommen, wie viele Stimmen in dem jeweiligen Musikstück erklingen.
\end{description}

\subsection{Einbindung von Künstlicher Intelligenz}
Für eine lange Zeit basierten AMT-Systeme größtenteils auf signal verarbeitenden Algorithmen,
wie zum Beispiel HMMs \titleref{subsec:hiddenMarkov}.
Doch auch diese Systeme waren nicht verschont durch den stetigen Anstieg von KI basierten Lösungswegen.
2010 bis 2012 wurden die ersten Ansätze von AMT-Systemen erstellt, welche maschinelles Lernen nutzten\cite{eyben2010universal}.
Oft wurden KI-Modelle nur für bestimmte Teilprobleme genutzt
wie Onset Detection, pitch estimation oder instrument classification.
Dieser Trend zog sich durch, wodurch heutzutage so gut wie jedes AMT-System eine Art von KI-Modell nutzt.

Einige AMT-Systeme nutzen auch mehrere KI-Modelle gleichzeitig.
Solch ein AMT-System wird auch einer 2016 veröffentlichen Arbeit beschrieben\cite{sigtia2016end}.
Es ist eins der ersten KI basierenden Ent-to-End AMT-Systeme, welches polyphone Musikstücke transkribieren kann.
Der KI-Ablauf des AMT-Systems ist folgendermaßen aufgebaut.
Ein CNN bekommt als Input ein Log-Mel-Spektrogramm des Audiosignals bereitgestellt
und entzieht diesem bestimmte Merkmale der Noten.
Diese Merkmale werden einem RNN weitergegeben, welches dadurch die zeitlichen Abhängigkeiten herausfiltert.
Als Nächstes wird durch Frame-wise Multi-Label Classification vorhergesagt, welche Noten zu welcher Zeit aktiv sind.
Somit bekommt man als Output eine Binärmatrix, wo der Pitch mit Abhängigkeit der Zeit angezeigt wird.
Das System wird mit dem MAPS Datensatz durch Supervised Learning trainiert.
Diese Arbeit stellt den drastischen übergang von heuristischen, regelbasierten AMT-Systemen
zu selbstlernenden AMT-Systemen dar.
Heutzutage sind CNNs und RNNs, oder ähnliche KI alternativen, kaum wegzudenken aus der Struktur von AMT-Systemen.

Viele intelligente und erfindungsreiche Personen haben sich über die Jahre mit AMT-Systemen beschäftigt.
Dadurch konnten nachfolgende Arbeiten neue Module und Ansätze für sich nutzen, um deren Entwicklung weiter voranzutreiben.
Auch heutzutage ist dies immer noch der Fall, wodurch es stets neue AMT-Systeme mit neuen Ansätzen gibt.
Diese neuen, auf KI basierenden Systeme, haben alle jeweils ihre eigenen Stärken und schwächen.
Auch wenn die Forschung von AMT-Systemen in den letzten Jahren zahlreiche Fortschritte erfuhr,
gibt es trotz dessen noch große Baustellen und ungelöste Aufgaben.
Im folgenden Kapitel wird genauer der momentane Stand von AMT-Systemen behandelt.
So werden anhand einiger unterschiedlichen Beispiele, verschiedene AMT Strukturen dargestellt.
Zudem werden die relevantesten Hindernisse in der heutigen automatischen Musiktranskription aufgegriffen.