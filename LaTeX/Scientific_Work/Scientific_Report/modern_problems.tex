\section{Hindernisse Moderner AMT-Systeme}
Trotz der Einführung von KI-Modellen bestehen weiterhin zahlreiche Probleme,
die bislang ungelöst oder nur unzureichend gelöst sind.
Zudem agieren KIs völlig anders als normale Algorithmen.
Dadurch entstehen einige zusätzliche Herausforderungen die behandelt werden müssen.
In den folgenden Abschnitten werden die größten und wichtigsten Probleme,
mit der sich die AMT Forschung momentan auseinandersetzt, aufgezählt.

\subsection{Onset Detection}\label{subsec:onset}
Die On-/Offset Erkennung von Noten wurde schon ausgiebig in zahlreichen Arbeiten behandelt.
Trotzdem ist diese noch nicht komplett akkurat.
Das liegt daran, dass On-/Offsets an Lautstärkesprüngen und spektralen Änderungen erkannt werden.
Bei polyphonen Musikstücken mit vielen verschiedenen Noten ist es schwieriger diese Unterschiede zu erkennen.
Lautstärkesprünge werden ungenauer, da häufig mehrere Noten während des Onsets einer bestimmten Note spielen.
Spektrale Änderungen stehen für Veränderungen der Energieverteilung.
Einer der ausschlaggebendsten Anteile ist die Spektrale Fluktuation.
Diese stellt den plötzlichen Anstieg von Energie in bestimmten Frequenzbändern dar.
Wenn mehrere einzelne Töne hintereinander, auf einem Klavier, gespielt werden, kann der Onset der gespielten Noten leicht ermittelt werden.
Bei Instrumenten, wie einer Geige, können dabei Probleme entstehen.
Auf Geigen können Noten gebunden gespielt werden, was zu einem Unterschied in der Frequenz,
jedoch nicht in dem Energie level führt, da die Stärke des Bogenstrichs gleichmäßig bleibt.
Zudem kann eine Note, durch Crescendo, zunächst leise gespielt werden und mit der Zeit an Lautstärke zunehmen.
Dieser Onset hat keinen Energie-Peak und somit erkennt das System hier auch nur schwierig eine scharfe Kante.
So welche Töne, welche keinen starken Einschlag haben, werden als nicht-perkussiv bezeichnet.
Ein weiteres Problem der Onset Erkennung sind nachwirkende Geräusche oder Störgeräusche.
Bleiben wir bei dem Beispiel einer Geige.
Wenn ein Ton gespielt wird und der Bogen von der Geige abgehoben wird entsteht eine Resonanz des Tons.
Das führt zu einem nachklingen des Tons, was sehr natürlich passieren kann,
jedoch meist kein beabsichtigtes Merkmal eines Tons ist.
Das KI-Modell sieht dieses nachklingen aber noch als der gespielte Ton und erkennt nicht perfekt das Offset des Tons.
Zudem könnten Töne von anderen Instrumenten verdeckt werden, wodurch das System den Onset nicht erkennt.
Ähnlich kann dies auch ein Problem darstellen, wenn im Audiosignal ein starkes Rauschen besteht.

\subsection{Polyphonie}
Durch die Nutzung von polyphonen Musikstücken sind, wie schon bei der Onset Erkennung angemerkt,
einige Probleme schwerwiegender und deutlicher geworden.
Ein weiteres Problem, das gezielt von diesen Anwendungsfällen abhängt, ist die Verwechselung von Noten im Audiosignal.
Einige Noten haben sehr ähnliche Frequenzen.
Wenn diese Noten gleichzeitig gespielt werden, ist es schwieriger für das System, die korrekten Noten herauszuhören.
Neuronale Netzwerke helfen hierbei deutlich, da diese nicht nur das Spektrum zur Analyse einbeziehen,
sondern auch auf einer zeitlichen und harmonischen Ebene den Kontext zuordnen können
und somit die wahrscheinlichsten nächsten Noten zurückgeben können.
Trotz dessen scheitert auch KI an der Einordnung der richtigen Noten, wenn die gespielten Noten
eine sehr ähnliche Frequenz besitzen oder die Akkorde zu unterschiedlich zu den Trainingsdaten sind.
In der Referenzierten Arbeit \cite{martak2022balancing} werden diese Probleme nochmals deutlicher aufgegriffen.

\subsection{Spezielle Instrumente}
Oft ist die Qualität der Transkription auch abhängig von den vertretenen Instrumenten in dem Audiosignal.
Ethnische Instrumente zum Beispiel sind Instrumente die einer bestimmten Kultur angehören
und in der westlichen Kultur weniger vertreten sind.
Dadurch gibt es auch weniger Datensätze, welche diese Instrumente beinhalten.
KIs brauchen Trainingsdaten und ohne diese können sie bestimmte Instrumente nicht richtig zuordnen.
Die meisten Datensätze zur Musiktranskription bestehen hauptsächlich aus Klavier und Geigen Noten.
Instrumente wie Flöten oder Orgeln können von diesen Noten gut abgeleitet werden,
da sich deren Struktur deutlich ähnelt.
Eine weitere Gruppe von Instrumenten die Probleme bei der Transkription bereitet sind elektronische Instrumente.
Wenn zum Beispiel eine E-Gitarre gespielt wird, kann diese Effekte nutzen,
welche nicht üblich in klassischen Klavier Datensätzen vorkommen.
Somit kann die KI diese Töne nicht korrekt erkennen.
Ein weiteres Instrument welches Schwierigkeiten bringt, ist der Gesang.
Jede Stimme ist einzigartig und vor allem nicht konstant.
Wenn ein Ton auf einem Klavier gespielt wird, besitzt dieser,
wenn das Klavier richtig gestimmt ist, immer die gleiche Frequenz.
Ein Mensch kann jedoch nicht jeden Ton immer komplett perfekt singen, wodurch eine große Varianz an Tönen entsteht.
Zudem verläuft der Klang einer Stimme von einer Note zur nächsten.
Es gibt nicht immer starke Peaks zur Onset Erkennung.
Gesang ist auch, wie die vorher genannten Instrumentgruppen, nicht sonderlich vertreten in größeren Datensätzen.
Die folgenden Paper konzentrieren ihren Fokus speziell auf das Thema des Gesangs in der Musiktranskription \cite{gu2023deep,gu2024automatic}.
Somit wurde eine Onset Erkennungsgenauigkeit von ungefähr 80\% festgestellt, für Gesang.

Die folgende Tabelle stellt einige traditionelle Instrumente aus verschiedenen ethnischen Gruppen dar.
Diese Tabelle ist nicht vollständig und dient ausschließlich dazu,
die Vielfalt der Instrumente in unserer Welt zu veranschaulichen.
\begin{table}[H]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Region} & \textbf{Instrumente} \\ \hline
Europa & Klavier, Violine, Gitarre, Flöte \\ \hline
Arabische Welt & Oud, Qanun, Nay, Darbuka \\ \hline
Türkei & Bağlama (Saz), Kemençe, Zurna, Davul \\ \hline
Persien / Iran & Tar, Setar, Santur, Tombak \\ \hline
Indien & Sitar, Tabla, Harmonium, Bansuri \\ \hline
China & Guzheng, Erhu, Pipa, Dizi \\ \hline
Japan & Koto, Shamisen, Shakuhachi, Taiko \\ \hline
Indonesien & Gamelan, Kendang, Suling, Rebab \\ \hline
Afrika & Kora, Djembe, Balafon, Mbira \\ \hline
\end{tabular}
\caption{Übersicht populärer Instrumente verschiedener Weltregionen}
\end{table}

\subsection{Notendauer}
Neben dem Onset einer Note muss auch erkannt werden, wie lange eine bestimmte Note gespielt wird.
Eine Schwierigkeit dabei ist der, im Abschnitt-\pref{subsec:onset} angesprochenen, Nachhall einer Note.
Dieser muss auseinandergehalten werden von der wirklichen Notendauer.
Es gibt jedoch bei einigen Instrumenten, wie zum Beispiel dem Klavier, ein spezielles Problem.
Wenn während des Spielens einer Note das Dämpferpedal eines Klaviers gedrückt wird, gibt es keinen klaren Punkt,
an dem der Übergang zwischen der Note und dem Nachhallen eindeutig eingeordnet werden kann.
Die Lautstärke sinkt dabei nicht abrupt, sondern nur langsam ab.
Natürlich ist dieses Problem in polyphonen Musikstücken deutlich stärker,
da sich dort die verschiedenen Nachhallphasen überlappen.
Dies ist eins der grundlegendsten Probleme, zusammen mit der Onset Erkennung \cite{jamshidi2024machine}.

\subsection{Reale Audioaufnahmen}
Ein perfektes AMT-System sollte sogar auf realen Audioaufnahmen 100\% Genauigkeit besitzen.
In der Realität funktioniert dies jedoch noch nicht, da Live-Audioaufnahmen Hintergrundrauschen enthält.
Dies war schon vor der Einführung von KI ein Problem und wurde durch die Einbindung
von KI-Modellen nicht sonderlich verbessert.
Das liegt daran, dass KIs mit isolierten Studioaufnahmen, wie es im MAESTRO Datensatz der Fall ist, trainiert werden.
Natürlich gibt es auch Datensätze mit realen Audioaufnahmen, jedoch bräuchte es dafür viel mehr Trainingsdaten und
vor allem auch eine große Anzahl von unterschiedlichen Hintergrundgeräuschen,
damit die KI auf alle Möglichkeiten trainiert wird.
Um dem entgegenzuwirken, können Studio-Datensätze wie MAESTRO durch Data-Augmentation variiert werden,
wodurch realistischere Audioaufnahmen erstellt werden können.
So wurden auch die Trainingsdaten in folgender Arbeit erzeugt \cite{kusaka2024mobile}.
Es wird Timber, Reverb, Noise variierte und die Qualität des Aufnahmegerätes angepasst.
So werden zahlreiche neue Datensätze kreiert, die passend auf die gegebenen Anwendungsfälle ausgelegt sind.

\subsection{Frame-basierte vs Event-basierte AMT-Systeme}
Die meisten AMT-Systeme sind Frame-basiert und nicht Event-basiert.
Frame-basiert bedeutet, dass die Analyse des Audiosignals in Zeitfenster, von ungefähr 20ms, aufgeteilt wird.
Ein KI-Modul, wie zum Beispiel ein RNN, analysiert diese Zeitfenster einzeln,
in Abhängigkeit von den vorherigen Zeitfenstern.
Falls nun zum Beispiel ein Onset einer Note genau zwischen zwei Zeitfenstern liegt,
wird dieser verschoben oder verzerrt, sodass das Onset klar in einem der gegebenen Zeitfenstern liegt.
Event-basierte AMT-Systeme hingegen fragen immer ab,
was das nächste musikalische Ereignis im Audiosignal ist und reagieren dementsprechend.
Somit müssen keine musikalischen Merkmale verschoben werden,
da sie ihre eigene Position besitzen und nicht abhängig von Zeitfenstern (Rastern) sind.
Dadurch entspricht der Output viel mehr dem Input, in relation zu der zeitlichen Abfolge.
Im Ergebnis sind Event-basierte AMT-Systeme somit Frame-basierten AMT-Systemen überlegen.
In der Realität sind die meisten AMT-Systeme trotzdem Frame-basiert.
Das liegt daran, dass Event-basierte AMT-Systeme eine sehr neue Entwickelung sind
und zudem auch schwerer zu implementieren sind.
Frame-basierte Methoden wurden ungefähr im Jahre 2000 entwickelt
und haben sich seitdem in zahlreichen Arbeiten durchgesetzt \cite{Martin1996, klapuri1998automatic}.
Dahingegen sind Event-basierte Methoden erst ungefähr 15 Jahre später entwickelt worden \cite{performance_rnn2017event}.
Dadurch waren Frame-basierte Methoden lange der Standard für AMT-Systeme.
Zudem ist die Umsetzung von Event-basierten AMT-Systemen komplizierter als Frame-basierte.
Das System muss jedes Event korrekt einschätzen und zuordnen.
Zum Beispiel könnte hintergrund Rauschen als Note erkannt werden und somit im
Gedächtnis eines RNNs, als Note, gespeichert werden.
In Event-basierenden Systemen wird eine Note nur einmal für die nachfolgenden Frames, in denen sie klingt, vorhergesagt.
Heißt es existiert eine falsche Note in der Transkription,
die auch nicht mehr von KI-Modellen oder anderen Algorithmen entfernt wird.
Bei der Frame-basierten Transkription hingegen wird eine Note in jedem Zeitfenster neu analysiert,
wodurch fehlerhafte Noten öfter erkannt werden.
Eine falsche Note wirkt sich ausschlaggebend auf das weitere Audiosignal aus,
da zum Beispiel RNNs diese Noten teilweise im Gedächtnis speichern und daraus Rückschlüsse auf zukünftige Noten führen.
Auch das Training von Event-basierten AMT-Systemen kann nicht parallel verlaufen,
sondern muss tokenweise abgearbeitet werden.
Deswegen sind Event-basierte AMT-Systeme eine größere Herausforderung.
Sie führen deutliche mehr Risiken mit sich, geben dafür aber auch ein besseres Ergebnis zurück, wenn alles perfekt funktioniert.
Da irgendwann ein perfektes AMT-System entstehen sollte ist der Übergang von
Frame zu Event mit der Zeit unausweichlich, da somit der Inhalt von Musikstücken
präziser wiedergeben werden kann und ein Ergebnis erzielt wird,
welches durch Frame-basierte transkription sonst nicht möglich wäre.
Die Forschung im Bereich der Event-basierten Transkription hat sich in den letzten Jahren deutlich intensiviert.

\subsection{Blackbox von KI-Modellen}
Das letzte Problem betrifft KI im generellen.
Ein neuronales Netz arbeitet mit sehr hochdimensionalen Räumen, die für uns Menschen nicht begreifbar sind.
Selbst wenn sich jemand alle Gewichtungen in einem neuronalen Netz anschauen würde,
würde dieser keinen Zusammenhang feststellen können.
Dies stellt auch ein Problem in der Musiktranskription dar, da somit nicht erfasst werden kann,
welche Einstellungen eine AMT-KI genau braucht,
damit sie alle Musikstücke perfekt transkribieren kann.
Um dieses Problem zu lösen, gibt es Methoden, welche die Blackbox ein wenig umgehen.
Das Forschungsgebiet der Explainable AI befasst sich damit,
KI-Modelle zu entwickeln, welche für den Menschen nachvollziehbar und verständlich sind.
Dies geschieht durch verschiedene Methoden und Darstellungen, welche den Denkfluss einer KI veranschaulichen sollen.
Einige von diesen Methoden finden ihre Anwendung auch in KI-basierten AMT-Systemen wieder.
Methoden die zur Nutzung in AMT-Systemen diskutiert werden, sind \enquote{Concept Activation Vectors},
\enquote{Layer-wise Relevance Propagation} und \enquote{Surrogat-Modelle}.
Diese wurden jedoch noch in keinem AMT-System richtig implementiert.
Musikalische Konzepte wie Akkorde oder Onsets sind nicht direkt lable bar, wie zum Beispiel Katzen oder Hunde,
da diese stark abhängig von spektralen und zeitlichen Mustern sind.
AMT-Systeme sind zeitlich-sequenziell und besitzen pro Zeitfenster mehrere Outputs.
Viele lokale Änderungen auf ein bestimmtes Zeitfenster, wie den dB-Wert ändern, haben einen globalen Einfluss.
Methoden, welche in AMT-Systemen angewendet werden,
haben dahingegen Eigenschaften, die durch die Struktur von AMT-Systemen profitieren.
Methoden die schon in AMT-Systemen eingesetz werden sind Saliency Maps, Feature-Visualisierung und Attention-Mapping.
Saliency Maps stellen dar, wie stark jedes Zeitfenster beeinflusst, das ein bestimmter Ton erkannt wurde.
Dies basiert auf dem Gradienten und ist besonders hilfreich bei CNN-basierten Modellen.
Feature-Visualisierung beobachtet konkret die einzelnen Neuronen jeder Ebene im neuronalen Netzwerk.
Es wird geschaut, welche Neuronen sehr stark bei bestimmten Inputs reagieren.
So kann erkannt werden, welche Teile des Netzes für welche Aufgaben verantwortlich sind
und ob vielleicht bestimmte Stellen des Netzes gar nicht genutzt werden.
Attention-Mapping nimmt den gegebenen Input und gewichtet diesen,
je nachdem welche Teile davon wichtig für eine bestimmte Vorhersage sind.
So wird erkannt, welche Zeitfenster den meisten Einfluss auf einen bestimmte Onset hatten.
Vor allem bei polyphonen Musikstücken können relevante zeitliche Zusammenhänge erkannt werden.
Ein gutes Beispiel von Attention-Mapping kann in folgender Arbeit gefunden werden \cite{cheuk2021revisiting}.
