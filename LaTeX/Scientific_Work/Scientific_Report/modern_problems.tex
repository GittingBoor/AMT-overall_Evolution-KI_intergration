\section{Hindernisse Moderner AMT-Systeme}
Trotz der Einführung von KI-Modellen bestehen weiterhin zahlreiche Probleme,
die bislang ungelöst oder nur unzureichend gelöst sind.
Zudem agieren KIs völlig anders als normale Algorithmen.
Dadurch entstehen einige zusätzliche Herausforderungen die behandelt werden müssen.
In den folgenden Abschnitten werden die größten und wichtigsten Probleme,
mit der sich die AMT Forschung momentan auseinandersetzt, aufgezählt.

\subsection{Onset Detection}\label{subsec:onset}
Die On-/Offset Erkennung von Noten wurde schon ausgiebig in zahlreichen Arbeiten behandelt.
Trotzdem ist diese noch nicht komplett akkurat.
Das liegt daran, dass On-/Offsets an Lautstärke sprüngen und spektralen Änderungen erkannt werden.
Bei polyphonen Musikstücken mit vielen verschiedenen Noten ist es schwieriger diese Unterschiede zu erkennen.
Lautstärke sprünge werden ungenauer, da häufig mehrere Noten während des Onsets einer bestimmten Note spielen.
Spektrale Änderungen stehen für Veränderungen der Energieverteilung.
Einer der ausschlaggebendsten Anteile ist die Spektrale Fluktuation.
Diese stellt den plötzlichen Anstieg von Energie in bestimmten Frequenzbändern dar.
Wenn mehrere einzelne Töne hintereinander, auf einem Klavier, gespielt werden, kann der Onset der gespielten Noten leicht ermittelt werden.
Bei Instrumenten, wie einer Geige, können dabei Probleme entstehen.
Auf Geigen können Noten gebunden gespielt werden, was zu einem unterschied in der Frequenz,
jedoch nicht in dem Energie level führt, da die Stärke des Bogenstrichs gleichmäßig bleibt.
Zudem kann eine Note, durch Crescendo, zunächst leise gespielt werden und mit der Zeit an Lautstärke zunehmen.
Dieser Onset hat keinen Energie-Peak und somit erkennt das System hier auch nur schwierig eine scharfe Kante.
So welche Töne, welche keinen starken Einschlag haben, werden als nicht-perkussiv bezeichnet.
Ein weiteres Problem der Onset erkennung sind nachwirkende Geräusche oder Störgeräusche.
Bleiben wir bei dem Beispiel einer Geige.
Wenn ein Ton gespielt wird und der Bogen von der Geige abgehoben wird entsteht eine Resonanz des Tons.
Das führt zu einem nachklingen des Tons, was sehr natürlich passieren kann,
jedoch meist kein beabsichtigtes Merkmal eines Tons ist.
Das KI-Modell sieht dieses nachklingen aber noch als der gespielte Ton und erkennt nicht perfekt das Offset des Tons.
Zudem könnten Töne von anderen Instrumenten verdeckt werden, wodurch das System den Onset nicht erkennt.
Ähnlich kann dies auch ein Problem darstellen, wenn im Audiosignal ein starkes Rauschen besteht.

\subsection{Polyphonie}
Durch die nutzung von polyphonen Musikstücken sind, wie schon bei der Onset Erkennung angemerkt,
einige Probleme schwerwiegender und deutlicher geworden.
Ein weiteres Problem, das gezielt von diesen Anwendungsfällen abhängt, ist die Verwechselung von Noten im Audiosignal.
Einige Noten haben sehr ähnliche Frequenzen.
Wenn diese Noten gleichzeitig gespielt werden, ist es schwieriger für das System, die korrekten Noten herauszuhören.
Neuronale Netzwerke helfen hierbei deutlich, da diese nicht nur das Spektrum zur analyse einbeziehen,
sondern auch auf einer zeitlichen und harmonischen Ebene den Kontext zuordnen können
und somit die wahrscheinlichsten nächsten Noten zurückgeben können.
Trotz dessen scheitert auch KI an der Einordnung der richtigen Noten, wenn die gespielten Noten
eine sehr ähnliche Frequenz besitzen oder die Akkorde zu unterschiedlich zu den Trainingsdaten sind.
In der folgenden Arbeit werden diese Probleme nochmals deutlicher aufgegriffen.
\cite{martak2022balancing}

\subsection{Spezifische Instrument}
Oft ist die Qualität der Transkription auch abhängig von den vertretenen Instrumenten in dem Audiosignal.
Ethnische Instrumente zum Beispiel sind Instrumente die einer bestimmten Kultur angehören
und in der westlichen Kultur weniger vertreten sind.
Dadurch gibt es auch weniger Datensätze, welche diese Instrumente beinhalten.
KIs brauchen Trainingsdaten und ohne diese können sie bestimmte Instrumente nicht richtig zuordnen.
Die meisten Datensätze zur Musiktranskription bestehen hauptsächlich aus Klavier und Geigen Noten.
Instrumente wie Flöten oder Orgeln können von diesen Noten gut abgeleitet werden,
da sich deren Struktur deutlich ähnelt.
Eine weitere Gruppe von Instrumenten die Probleme bei der Transkription bereitet sind elektronische Instrumente.
Wenn zum Beispiel eine E-Gitarre gespielt wird, kann diese Effekte nutzen,
welche nicht üblich in klassischen Klavier Datensätzen vorkommen.
Somit kann die KI diese Töne nicht korrekt erkennen.
Das letzte Instrument welches schwierigkeiten bringt, ist der Gesang.
Jede Stimme ist einzigartig und vor allem nicht konstant.
Wenn ein Ton auf einem Klavier gespielt wird, besitzt dieser,
wenn das Klavier richtig gestimmt ist, immer die gleiche Frequenz.
Ein Mensch kann jedoch nicht jeden Ton immer komplett perfekt singen, wodurch eine große varianz an Tönen entsteht.
Zudem verläuft der Klang einer Stimme von einer Note zur nächsten.
Es gibt nicht immer starke Peaks zur Onset Erkennung.
Gesang ist auch, wie die vorher genannten Instrumentgruppen, nicht sonderlich vertreten in größeren Datensätzen.
Die folgenden Paper konzentrieren ihren Fokus speziell auf das Thema des Gesangs in der Musiktranskription.
\cite{gu2023deep,gu2024automatic}
Somit wurde eine Onset Erkennungsgenauigkeit von ungefähr 80\% festgestellt, für Gesang.

Die folgende Tabelle stellt einige traditionelle Instrumente aus verschiedenen ethnischen Gruppen dar.
Diese Tabelle ist nicht vollständig und dient ausschließlich dazu,
die Vielfalt der Instrumente in unserer Welt zu veranschaulichen.
\begin{table}[H]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Region} & \textbf{Instrumente} \\ \hline
Europa & Klavier, Violine, Gitarre, Flöte \\ \hline
Arabische Welt & Oud, Qanun, Nay, Darbuka \\ \hline
Türkei & Bağlama (Saz), Kemençe, Zurna, Davul \\ \hline
Persien / Iran & Tar, Setar, Santur, Tombak \\ \hline
Indien & Sitar, Tabla, Harmonium, Bansuri \\ \hline
China & Guzheng, Erhu, Pipa, Dizi \\ \hline
Japan & Koto, Shamisen, Shakuhachi, Taiko \\ \hline
Indonesien & Gamelan, Kendang, Suling, Rebab \\ \hline
Afrika & Kora, Djembe, Balafon, Mbira \\ \hline
\end{tabular}
\caption{Übersicht populärer Instrumente verschiedener Weltregionen}
\end{table}

\subsection{Notendauer}
Neben dem Onset einer Note muss auch erkannt werden, wie lange eine bestimmte Note gespielt wird.
Eine Schwierigkeit dabei ist der, im Abschnitt-\titleref{subsec:onset} angesprochenen, Nachhall einer Note.
Dieser muss auseinandergehalten werden von der wirklichen Notendauer.
Es gibt jedoch bei einigen Instrumenten, wie zum Beispiel dem Klavier, ein spezielles Problem.
Wenn während des Spielens einer Note das Dämpferpedal eines Klaviers gedrückt wird, gibt es keinen klaren Punkt,
an dem der Übergang zwischen der Note und dem Nachhallen eindeutig eingeordnet werden kann.
Die Lautstärke sinkt dabei nicht abrupt, sondern nur langsam ab.
Natürlich ist dieses Problem in polyphonen Musikstücken nochmal deutlich stärker,
da sich dort die verschiedenen Nachhallphasen überlappen.
Dies ist eins der grundlegendsten Probleme, zusammen mit der Onset Erkennung.
\cite{jamshidi2024machine}

\subsection{Reale Audioaufnahmen}
Ein perfektes AMT-System sollte sogar auf realen Audioaufnahmen 100\% Genauigkeit besitzen.
In der Realität klappt das aber nicht, da Live-Audioaufnahmen immer Hintergrundrauschen besitzt.
Dies war schon vor der Einführung von KI ein Problem und wurde durch die Einbindung
von KI-Modellen nicht sonderlich verbessert.
Das liegt daran, dass KIs mit isolierten Studioaufnahmen, wie es im MAESTRO Datensatz der Fall ist, trainiert werden.
Natürlich gibts auch Datensätze mit realen Audioaufnahmen, jedoch bräuchte es dafür viel mehr Trainingsdaten und
vor allem auch eine große Anzahl von unterschiedlichen Hintergrundgeräuschen,
damit die KI auf alle Möglichkeiten trainiert wird.
Um dem entgegenzuwirken, können Studio-Datensätze wie MAESTRO durch Data-Augmentation variiert werden,
wodurch realistischere Audioaufnahmen erstellt werden können.
So wurden auch die Trainingsdaten in folgender Arbeit erzeugt.
\cite{kusaka2024mobile}
Es wird Timber, Reverb, Noise variierte und die Qualität des Aufnahmegerätes, zu zum Beispiel der eines Smartphones, angepasst.
So werden zahlreiche neue Datensätze kreiert, die passend auf die gegebenen Anwendungsfälle ausgelegt sind.

% HIER WEITER KORRIGIEREN

\subsection{Frame-basierte vs Event-basierte AMT-Systeme}
Die meisten AMT-Systeme sind Frame-basiert und nicht Event-basiert.
Frame-basiert bedeutet, das die Analyse des Audiosignals in Zeitfenster, ungefähr 20ms, aufgeteilt wird.
Für jedes Zeitfenster wird somit der momentane Stand das Audiosignal von den verschiedenen Modulen analysiert.
Falls nun zum Beispiel ein Onset einer Note genau zwischen zwei Zeitfenstern liegt,
wird dieser verschoben oder verzerrt, sodass dieser mit den gegebenen Zeitfenstern übereinstimmt.
Event-basierte AMT-Systeme hingegen fragen immer ab,
was das nächste musikalische Ereignis im Audiosignal ist und reagieren dementsprechend.
Dadurch entspricht der Output viel mehr dem Input, in relation zu der zeitlichen Abfolge.
Im Ergebnis sind Event-basierte AMT-Systeme somit Frame-basierten AMT-Systemen überlegen.
Jedoch ist in der Realität trotzdem fast jedes AMT-System Frame-basiert.
Das liegt daran, dass Event-basierte AMT-Systeme eine sehr neue Entwickelung sind
und zudem auch schwerer zu implementieren ist.
Frame-basierte Methoden wurden ungefähr im Jahre 2000 entwickelt
und haben sich seitdem in zahlreichen Arbeiten durchgesetzt.
\cite{Martin1996, klapuri1998automatic}
Dahingegen sind Event-basierte Methoden erst ungefähr 15 Jahre später entwickelt worden.
\cite{performance_rnn2017event}
Deshalb wurden Frame-basierte Methoden lange als der Standard angesehen.
Zudem ist die Umsetzung von Event-basierten AMT-Systemen schwerer als Frame-basierte.
Das System muss korrekt jedes Event einschätzen un zuordnen, ist es ein Onset ein Offset oder doch nur Rauschen?
Wenn es nun einen Fehler macht, wird sich das,
im gegensatz zu Frame-basierten, auf das ganze weitere Audiosignal auswirken.
Auch das Training kann nicht parallel verlaufen, sondern muss token weise abgearbeitet werden.
Diese Gründe, und noch einige andere, machen Event-basierte AMT-Systeme zu einer viel größeren Herausforderung.
Jedoch ist der übergang von Frame zu Event mit der Zeit irgendwann nötig, da man somit den Inhalt der Musikstücke
viel präziser wiedergeben kann.
Die Forschung in dem Event-basierten Methoden steigt auch in den letzten Jahren immer mehr an.

\subsection{Blackbox von KI-Modellen}
Das letzte Problem betrifft KI im generellen.
Ein neuronales Netz arbeitet mit sehr hochdimensionalen Räumen, die für uns Menschen nicht begreifbar sind.
Selbst wenn jemand sich die Millionen und mehr Gewichtungen anschauen würde,
würde dieser in diesen keinen Zusammenhang feststellen können.
In der Musiktranskription ist dies auch ein Problem, da somit nicht erfasst werden kann,
welche Einstellungen eine automatischen Musiktranksriptions KI genau braucht,
damit sie alle Musikstücke perfekt transkribieren kann.
Jedoch gibt es auch Methoden, welche die Blackbox ein wenig umgehen.
Im Forschungsgebiet der Explainable AI gibt es einige Methoden,
welche auch in KI-basierten AMT-Systemen, verwendet werden.
Unter diesen fallen Saliency Maps, Feature-Visualisierung und Attention-Mapping.
Concept Activation Vectors, Layer-wise Relevance Propagation, Surrogat-Modelle sind andere Explainable AI Methoden,
die zur nutzung in AMT-Systemen diskutiert werden, aber noch nicht richtig integriert sind.
Dies liegt an mehreren Gründen.
Musikalische Konzepte wie Akkorde oder Onsets sind nicht direkt lablebar, wie zum beispiel Katzen oder Hunde,
da diese stark abhängig von spektralen und zeitlichen Mustern sind.
AMT-Systeme sind zeitlich-sequenziell und besitzen pro Zeitfenster mehrere Outputs.
Viele lokale Änderungen auf ein bestimmtes Zeitfenster, wie den dB-Wert ändern, haben einen globalen Einfluss.
Methoden, welche in AMT-Systemen angewendet werden,
haben dahingegen Eigenschaften, die durch die Struktur von AMT-Systemen profitieren.
Saliency Maps stellen dar, wie stark jedes Zeitfenster im Spektrogramm,
beeinflusst hat, das zum Beispiel ein bestimmter Ton erkannt wurde.
Dies basiert auf dem Gradienten und ist besonders hilfreich bei CNN-basierten Modellen.
Feature-Visualisierung beobachtet konkret die einzelnen Neuronen jeder Ebene im neuronalen Netzwerk.
Es wird geschaut, welche neuronen sehr stark bei bestimmten Inputs reagieren.
So kann man erkennen, welche Teile des Netzes für welche Aufgaben verantwortlich sind
und ob vielleicht bestimmte stellen des Netzes gar nicht genutzt werden.
Attention-Mapping nimmt den gegebenen Input und gewichtet diesen,
je nachdem welche Teile davon wichtig für eine bestimmte Vorhersage sind.
So erkennt man, welche Zeitfenster den meisten Einfluss auf zum Beispiel einen bestimmte Onset hatten.
Vor allem bei polyphonen Musikstücken kann man relevante zeitliche Zusammenhänge erkennen.
Ein gutes Beispiel von Attention-Mapping findet man in folgender Arbeit.
\cite{cheuk2021revisiting}
