\section{Einleitung}
\subsection{Automatische Musiktranksription}
Musik war schon immer ein zentraler Bestandteil unserer Gesellschaft.
Während früher musikalische Werke meist mündlich überliefert wurden,
entwickelte sich während des Mittelalters die Notenschrift, welche wir auch heutzutage noch nutzen.
Diese Notenschrift ermöglichte es, Musikstücke einfacher zu erlernen und Musik einem breiteren Publikum zugänglich zu machen.
Durch die Digitalisierung erhielten \enquote{Digital Audio Workstations} zunehmend Einzug in die Musikproduktion,
wodurch Notenblätter irrelevanter wurden und somit weniger Musikstücke in Notenschrift übersetzt wurden.

Ungefähr gleichzeitig begann zudem die Forschung an AMT-Systemen (Automatic Music Transcription).
AMT ist ein Prozess, zur automatischen Transkription von Audiospuren.
Als Input wird ein Audiosignal gegeben, welches durch verschiedene Prozesse umgewandelt wird.
Diese Prozesse analysieren die Eigenschaften der Noten, zum Beispiel die Frequenz, Lautstärke und vieles mehr.
Dadurch entsteht als Output eine MIDI-Datei, ein Notenblatt oder andere Daten,
die das gegebene Musikstück in Schriftform darstellen.

\subsection{Grundlegende Begriffe in AMT-Systemen}
Bevor wir tiefer in das Forschungsfeld der automatischen Musiktranskription eintreten,
sollen zunächst einige zentrale Begriffe erläutert werden.
Diese sind für das grundlegende Verständnis von AMT-Systemen unverzichtbar,
da sie in nahezu allen Ansätzen Anwendung finden, mit Ausnahme der Polyphonie,
welche in frühen Systemen oftmals noch nicht berücksichtigt wurde.
Darüber hinaus stellen sie die wesentlichen Bausteine dar,
wie etwa der Beginn und das Ende von Noten, die Behandlung mehrstimmiger Musikstücke und zuletzt der Input und Output.

\begin{description}[style=nextline]
\item[Onsets und Offsets]\label{itm:onset_offset}
Onsets beschreiben den exakten Zeitpunkt, an dem eine Note beginnt.
Dementsprechend zeigen Offsets den Zeitpunkt, an dem diese endet.
Onsets werden in der Signalverarbeitung typischerweise durch plötzliche Änderungen im Audiosignal erkannt.
Durch einen starken Anstieg der Amplitude, Änderungen im Spektrum oder plötzliche Energiezunahmen im Frequenzbereichen.
Offsets markieren den Zeitpunkt, an dem die im Onset erfassten Merkmale nicht mehr vorhanden sind und die Note endet.
Die präzise Erkennung von Onsets und Offsets ist essenziell,
um die Dauer einer Note korrekt zu bestimmen und eine vollständig korrekte Notenschrift zu erzeugen.
Fehler in der Erkennung können zu falscher Notenlänge, fehlenden Noten oder zusätzlichen Phantomnoten führen.
In späteren Modellen, wie RNNs, können falsche Onsets und Offsets auch zu Folgefehler führen.

Die folgende Darstellung verdeutlicht,
wie musikalische Ereignisse anhand von Änderungen in der Amplitude erkannt werden können.
Ein deutlicher Anstieg der Amplitude markiert den Onset einer Note.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Graphics/Onset_detection}
    \caption[Noten Onsets]{3 Noten Onsets mit Abhängigkeit von Amplitude und Zeit \cite{brown2016patternvep}.}
    \label{fig:onsets}
\end{figure}
\end{description}

\begin{description}[style=nextline]
\item[Polyphonie]\label{itm:polyphonie}
Polyphonie beschreibt in der Musik die Mehrstimmigkeit.
Das bedeutet, dass ein Musikstück mehrere Stimmen gleichzeitig wiedergibt.
In der automatischen Musiktranskription muss das System diese gleichzeitig erklingenden Stimmen erkennen, voneinander unterscheiden und den jeweiligen Stimmen zuordnen.
Je mehr Stimmen ein Musikstück besitzt, desto schwieriger ist die Transkription.
Ein klassisches Beispiel für polyphone Musik sind Klavierstücke, Orchesterwerke oder Chormusik, bei denen oft viele Noten gleichzeitig erklingen.
\end{description}

\begin{description}[style=nextline]
\item[Spektrogramme]\label{itm:spektrogramm}
In der automatischen Musiktranskription wird das Input Audiosignal immer zunächst in ein Spektrogramm umgewandelt.
Spektrogramme zeigen den zeitlichen Verlauf des Frequenzspektrums eines Audiosignals.
Es gibt verschiedene Arten von Spektrogrammen.
Zwei Spektrogrammtypen, die häufig genutzt werden, sind CQT-Spektrogramme und Log-Mel-Spektrogramme.
CQT steht für Constant-Q Transform und in diesem Spektrogramm werden die Frequenzachsen logarithmisch aufgelöst.
Zudem bleibt der Q-Faktor konstant, dieser beschreibt das Verhältnis von Frequenz zu Brandbreite.
Das Log-Mel-Spektrogramm wird zunächst mit einer Short-Time Fourier Transform (STFT) gebildet.
STFT ist eine Methode, um ein Signal als ein Frequenzspektrum darzustellen.
Anschließend wird die lineare Frequenzachse auf eine Mel-Skala projiziert.
Das menschliche Gehör kann zum Beispiel 200--400Hz feiner als 5000--5200Hz wahrnehmen.
Die Mel-Skala sorgt dafür, das sich das Spektrogramm dem menschlichen Gehör anpasst.
Insgesamt ist ein CQT-Spektrogramm besser für die automatische Musiktranskription,
da in diesem die Töne feiner unterschieden werden können.
Jedoch werden in neueren KI-Modellen zum Großteil Log-Mel-Spektrogramme genutzt.
Diese sind robuster und funktionieren, vor allem in Transformer-basierte KI-Modellen, zuverlässiger.

Im Folgenden sind vier verschiedene Spektrogramme zu sehen,
welche alle auf den identischen Ausschnitt aus Bachs WTK I, Nr.5 \cite{bach_wtk1_nr5} basieren.
Oben links ist ein normales lineares Spektrogramm zu sehen,
welches ausschließlich auf dem STFT basiert und eine lineare Frequenzskala besitzt.
Rechts daneben ist ein Log-Spektrogramm,
bei dem die Amplitudenwerte des STFT logarithmisch in Dezibel skaliert wurden,
wodurch der Dynamikumfang im niedrigen Amplitudenbereich erhöht wird.
Das \enquote{MelSpec} steht für Log-Mel-Spektrogramm und unten rechts wird ein CQT-Spektrogramm angezeigt.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Graphics/different_spectrograms}
    \caption[Vergleich 4 Spektrogramme]{Vergleich von vier Spektrogrammen basierend auf  \cite{cheuk2020impact}.}
    \label{fig:different_specs}
\end{figure}
\end{description}

\begin{description}[style=nextline]
\item[MIDI-Dateien]\label{itm:midi}
Notenschrift als Input für ein Computerprogramm, Synthesizer oder ähnliches ist unhandlich.
Zunächst müssten die gespielten Noten stets in Notenschrift umgewandelt und anschließend in anderen Programmen analysiert werden.
Dieser Prozess ist sehr aufwändig.
Eine bessere Lösung wäre eine Datenschreibweise,
bei der alle wichtigen Informationen der Noten übersichtlich aufgeschrieben sind.
MIDI-Dateien sind dafür perfekt geeignet.

MIDI (Musical Instrument Digital Interface) ist ein Standartprotokoll zur Kommunikation zwischen
elektronischen Musikinstrumenten, Computern und anderen Geräten wie zum Beispiel Synthesizern.
Dieses Protokoll wurde erstmals 1983 eingeführt und wurde schnell zu einem Standard in der digitalen Musikindustrie \cite{smith1983midi}.
In MIDI-Dateien werden Daten von Tönen gespeichert,
welche zum Beispiel zuvor von einem elektronischen Instrument gespielt wurden oder durch AMTs erfasst wurden.

In MIDI-Dateien werden folgende Daten gespeichert:
\begin{enumerate}
    \item \textbf{MIDI Header-Chunk (MThd):} Enthält grundlegende Informationen zur Struktur der Datei:
    \begin{itemize}
        \item Dateiformat (Formattyp: 0 = eine Spur, 1 = mehrere synchrone Spuren, 2 = unabhängige Spuren)
        \item Anzahl der folgenden MTrk-Blöcke
        \item Zeitauflösung (Ticks pro Viertelnote)
    \end{itemize}

    \item \textbf{MIDI Track-Chunks (MTrk):} Jede Spur enthält eine zeitlich sortierte Liste von MIDI-Events:
    \begin{itemize}
        \item \textbf{MIDI-Events:}
        \begin{itemize}
            \item Onset / Offset
            \item Lautstärkeänderung (Control Change)
            \item Angabe des spielenden Instruments (Program Change)
            \item Tonhöhenänderung (Pitch)
            \item Druckstärke pro Taste (Aftertouch / Polyphonic Key Pressure)
        \end{itemize}
        \item \textbf{Meta-Events):}
        \begin{itemize}
            \item Tempo in Mikrosekunden pro Viertelnote (Set Tempo)
            \item Taktart (Time Signature)
            \item Tonart (Key Signature)
            \item Spurname (Track Name)
            \item Liedtext (Lyrics)
            \item Markierungen (Markers)
            \item Ende der Spur (End of Track)
        \end{itemize}
        \item \textbf{Systemexklusive Ereignisse (SysEx):}
        \begin{itemize}
            \item Herstellerspezifische Daten wie Synthesizer-Voreinstellungen oder Spezialbefehle
        \end{itemize}
    \end{itemize}

    \item \textbf{Delta-Time:} Gibt die Zeit in Ticks an, die seit dem letzten Ereignis vergangen ist:
    \begin{itemize}
        \item Ermöglicht die genaue zeitliche Positionierung jedes MIDI-Ereignisses
        \item Grundlage für das Timing und die rhythmische Struktur der Datei
    \end{itemize}
\end{enumerate}
Am wichtigsten sind dabei die MTrk-Blöcke, in denen die Daten der einzelnen Noten gespeichert werden.
Dabei stellt ein Track die Eventliste einer ganzen Stimme dar,
wie zum Beispiel die Melodiestimme, eine Violine, die Pedalsteuerung eines Klaviers oder Metadaten.
Es fällt auf das diese vier Beispiele alle sehr unterschiedliche Aufgaben und Bedeutungen haben.
Das liegt daran, dass in MIDI-Dateien zusammenhängende Funktionen gespeichert werden und nicht nur Musiknoten.

MIDI-Dateien kamen auch der Forschung für AMT-Systeme sehr gelegen,
da nun ein standardisiertes Output-Format für diese Programme zur Verfügung stand.
Später werden diese zudem sehr essenziell bei dem Training KI basierter AMT-Systeme \cite{telila2025cnn}.

Im folgenden Bild wird ein Auszug einer MIDI-Datei, mit zwei Noten, angezeigt.
Darunter ist ein Piano-Roll abgebildet, zugehörig zu oben genannten Noten.
Ein Piano-Roll ist eine grafische Darstellung von Musiknoten über die Zeit.
Dabei gibt die Position die Tonhöhe an und die Länge die Spieldauer.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Graphics/midi_pianoroll}
    \caption[MIDI-Datensatz]{MIDI-Datensatz mit zugehörigem Piano-Roll. Ausschnitt aus der Varbeitungspipeline des MT3-Modells  \cite{mt3colab}.}
    \label{fig:mt3_output_tokens}
\end{figure}
\end{description}

\subsection{Herausforderungen und Hindernisse}
Anstatt durch sein Gehör selber diese Musikstücke in Notenschrift zu übertragen,
würde ein AMT-System diese Aufgabe übernehmen.
Dieses Ziel war jedoch schwer zu erreichen,
da Musik mehrdimensional, durch Aspekte wie zeitlicher Abfolge, Tonhöhe und Polyphonie, ist.
Vor allem bei polyphonen Musikstücken haben herkömmliche Algorithmen häufig Schwierigkeiten.
In diesen Fällen müssen AMT-Systeme mehrere verschiedene Stimmen gleichzeitig analysieren und
im späteren die jeweiligen Töne voneinander differenzieren um diese dann eindeutig einem Instrument zuordnen zu können.
Ein weiteres Problem ist die Individualität jedes Musikstückes.
In realen Aufnahmen können leichtes Rauschen,
kleine Spielfehler oder stilistische Mittel wie Vibrato auftreten,
die je nach Interpreten unterschiedlich klingen.
Zudem sind die meisten AMT-Modelle auf die westlichen Tonleiter trainiert.
Dies kann zu Problemen führen, wenn zum Beispiel
arabische oder indische Musikstücke transkribiert werden sollen.

In folgender Tabelle sind einige Regionen aufgelistet und deren Tonleiter, die diese benutzen.
Die Tabelle ist nicht vollständig, auf der Welt gibt es noch viele weitere Regionen, welche auch ihre ganz eigenen Tonleiter nutzen.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Region} & \textbf{Typische Skalen} \\ \hline
    Europa & Dur, Moll, Kirchentonarten, Pentatonik \\
    \hline
    Arabische Welt & Maqām Rast, Maqām Bayati, Maqām Hijaz, Maqām Kurd, Maqām Nahawand \\
    \hline
    Türkei & Hicaz, Nihavent, Hüseyni, Segah, Uşşak \\
    \hline
    Persien / Iran & Shur, Mahur, Segah, Homayun, Nava \\
    \hline
    Indien & Rāga Yaman, Bhairav, Todi, Kafi, Bhupali \\
    \hline
    China & Gong, Shang, Jue, Zhi, Yu \\
    \hline
    Japan & Hirajoshi, In-sen, Yo, Kumoi \\
    \hline
    Indonesien & Sléndro, Pélog \\
    \hline
    Afrika & Pentatonik, Hexatonik, Balafon-Skalen \\
    \hline
    \end{tabular}
    \caption{Übersicht typischer Skalen/Tonleiter verschiedener Weltregionen}
\end{table}

\subsection{Anwendungsfelder und Vorteile}\label{subsec:praktisch}
AMT kann auch häufig bei anderen Problemen helfen
oder in zahlreichen Bereichen \enquote{Quality-of-Life-Changes} bringen.
Zum einen kann der Musikunterricht spannender und interaktiver gestaltet werden.
Es würde eine breitere Auswahl von Musikstücken geben, die den Schülern angeboten werden können,
wodurch diese durch individuell angepasste Musikstücke mehr Spaß und Ehrgeiz beim Lernen entwickeln.
Zudem können die gespielten Musikstücke der Schüler während des spielens transkribiert werden,
wodurch frühzeitig erkannt wird, wo der jeweilige Schüler noch Verbesserungsmöglichkeiten aufweist und Feedback erhalten muss.
Grundsätzlich können deutlich mehr Musikstücke transkribiert werden,
wodurch sich große Archive aufbauen lassen.
Ein größeres Interesse an Musik wird geweckt, da Musikstücke von beliebten Serien, Filmen oder Spielen
leichter für deren Musikbegeisterte Zielgruppe zugänglich sind.
Alleine dadurch, dass KI-Modelle Musikstücke besser verstehen,
können darauf aufbauend weitere Tools für die Musikproduktion entwickelt werden.
Auch andere Anwendungsfelder von Künstlicher Intelligenz würden dadurch profitieren.
KI-generierte Musik würde verbessert werden, da die KI selber ein besseres Verständnis der Musik entwickelt.
Audiobasierte Suchmaschinen könnten gewünschte Musikstücke oder bestimmte Videos präziser finden.

\subsection{AMT und Künstliche Intelligenz}
Um diese musikalische Vielfalt zu bewältigen, wird heutzutage Künstliche Intelligenz und Machine Learning eingesetzt.
Im Gegensatz zu Algorithmen ist KI flexibler und kann sich dadurch besser
auf kleine Abweichungen in Musikstücken einstellen.
Ein Audiosignal kann entweder im Studio produziert oder im Alltag aufgenommen werden.
Alltag aufgenommene Signale werden, in dieser Arbeit, als reale Audiosignale bezeichnet.
Reale Audiosignale beinhalten immer eine gewisse Menge an Rauschen.
Das kann bei der automatischen Musiktranskription für Schwierigkeiten sorgen,
da Algorithmen diese als zusätzliche Noten ansehen oder richtige Noten dadurch nicht erkennen könnten.
KIs können sich besser an die fehlerhaften Geräusche von Rauschen anpassen,
da neuronale Netzwerke mit genau diesen fehlerbehafteten Audiosignalen trainiert werden können.
Somit können AMT-Systeme besser angepasst werden an einen realistischen Alltagsgebrauch.

Auch die Mehrdimensionalität von Musik kann KI deutlich besser bewältigen als Algorithmen.
Neuronale Netze besitzen eine mehrdimensionale Struktur, die es ihnen ermöglicht,
verschiedene Muster, Stimmen und Eigenschaften zu erlernen \cite{graves2007multi}.
Auf der anderen Seite müssen klassische Algorithmen diese verschiedenen Dimensionen
explizit modellieren und sind nicht in der Lage, Muster selbstständig zu erkennen.
Sie folgen nur den fest einprogrammierten Anweisungen und sind somit nicht anpassungsfähig.

Es gibt verschiedene KI-Modelle, welche für AMT-Systeme in Frage kommen.
In diesem Abschnitt werden die am weitesten verbreiteten KI-Modelle aufgegriffen.
Weitere potenzielle KI-Modelle und der heutige
\enquote{State of the Art} werden im Abschnitt-\pref{sec:ki_integration} ausführlich erläutert.
Die meisten AMT-Systeme nutzen Recurrent Neural Networks (RNN) und Convolutional Neural Networks (CNN) \cite{Boeck2012}.
Diese Module bilden keine eigenständigen Systeme, sondern lassen sich flexibel innerhalb eines Systems kombinieren.
Da es keine objektiv richtigen oder falschen Notenfolgen gibt, ist der Einsatz von Supervised Learning am effizientesten.
Dementsprechend ist es notwendig ein zuverlässiges und großes Datenset aus Audiodateien und deren zugehörigen MIDI-Dateien zu besitzen.

CNNs können gut räumliche Strukturen erkennen.
Das ist sehr hilfreich bei der Analyse von Spektrogrammen.
Da das Input Audiosignal in ein Spektrogramm umgewandelt wird,
kann das CNN diese bildliche Darstellung analysieren.
Durch die Analyse von dem Spektrogramm können gewisse Frequenzmuster erkannt werden,
die darauffolgend einem bestimmten Instrument zugeordnet werden können.
Das ist vor allem dabei hilfreich, die verschiedenen Stimmen der jeweiligen Instrumente voneinander zu differenzieren \cite{han2016deep}.

RNNs hingegen spezialisieren sich darauf, mithilfe eines Gedächtnis, zeitliche Abläufe zu verstehen.
In Musikstücken werden zahlreiche Noten hintereinander gespielt, welche harmonisch aufeinander aufbauen.
Das RNN verarbeitet die jeweiligen Sequenzen und merkt sich die Informationen der schon gespielten Noten,
um die darauffolgenden Noten besser einordnen zu können.
So lässt sich die harmonische Struktur des Musikstückes analysieren.
Auch der Rhythmus und vorhandene Vorzeichen lassen sich dadurch interpretieren \cite{Boeck2012}.

In den meisten AMT-Systemen werden RNNs und CNNs kombiniert.
Zunächst verarbeitet das CNN das Spektrogramm und extrahiert dabei musikalische Merkmale.
Darauf folgend verarbeitet das RNN diese Merkmale und bildet konkrete Noten mit deren einzelnen Eigenschaften.
\begin{enumerate}
    \item \textbf{CNN}\\
    Extrahiert folgende Merkmale aus dem Spektrogramm:
    \begin{itemize}
        \item Frequenzverteilungen und spektrale Muster
        \item Tonhöhenlage und damit verbundene Obertöne
        \item Klangfarbe einzelner Instrumente
        \item Energieverteilung, unter anderem zur Erkennung von Toneinsätzen
        \item Harmonische Strukturen wie Akkordfolgen
    \end{itemize}

    \item \textbf{RNN}\\
    Verarbeitet auf Basis dieser Merkmale die zeitliche Abfolge und erkennt dabei folgende Eigenschaften:
    \begin{itemize}
        \item Reihenfolge und Übergänge musikalischer Ereignisse
        \item Beginn und Ende einzelner Töne zur Bestimmung der Notendauer
        \item Rhythmische Muster und zeitliche Gruppierungen
        \item Musikalische Phrasen mit zusammenhängender Struktur
        \item Wiederholungen, Themen oder längere Abhängigkeiten im Verlauf
    \end{itemize}

    \item \textbf{Output} \\
    Gibt das transkribierte Musikstück in strukturierter Form aus:
    \begin{itemize}
        \item Als MIDI-Datei mit exakten Noteninformationen
    \end{itemize}
\end{enumerate}
Nachdem die KI-Modelle die Daten verarbeitet haben, kann die erhaltene MIDI-Datei mit einem externen Programm
zu standardisierten Musiknoten transkribiert werden.

\subsection{Motivation, Zielsetzung und Ablauf}
Künstliche Intelligenz ist momentan eins der aktivsten Forschungsfelder.
Durch KIs wurden und werden noch viele grundlegende Probleme
und Algorithmen komplett umstrukturiert, verworfen oder gelöst.
Ein perfektes Beispiel dafür ist AlphaDev von DeepMind \cite{mankowitz2023faster},
welches durch Reinforcement Learning ein neues, effizienteres Sortier- und Hashverfahren entwickelt hat.
Ein Forschungsgebiet, das unter anderem auch sehr stark durch die Einführung von KI profitiert hat,
ist die automatische Musiktranskription.
Während früher noch Modelle und Algorithmen genutzt worden, um AMT-Systeme umzusetzen,
werden jetzt überwiegen verschiedene KI-Modelle genutzt,
um unter anderem Spektrogramme zu analysieren und um den Aufbau des Musikstückes zu identifizieren.

Durch die integration von KI in AMT-Systemen rückt die Vorstellung von AMT-Systemen,
welche im Alltag genutzt werden auch immer näher.
Sobald das erste AMT-System praxisgeeignet ist, werden sich, wie im Abschnitt-\pref{subsec:praktisch} angeschnitten,
viele neue Einsatzmöglichkeiten ermöglicht.
Dadurch werden andere Forschungsgebiete gefördert, momentane Aufgaben vereinfacht und das Bildungswesen verbessert.

Deshalb wird der Schwerpunkt dieser wissenschaftlichen Vertiefung darin liegen,
auf die Funktion von KI-Modellen in AMT-Systemen.
Diese Analyse wird sich aus drei Teilen zusammenfügen.
Zunächst werden die heutigen Schwachstellen moderner AMT-Systeme aufgelistet.
Als Nächstes werden die KI-Systeme beschrieben,
welche grundlegend in AMT-Systemen genutzt werden und welche Vorteile diese mit sich bringen.
Zuletzt werden zwei neu zeitige AMT-Systeme vorgestellt,
welche beide eine starke Auswirkung auf dieses Forschungsgebiet haben.

Um das Thema automatische Musiktranskription zu vervollständigen,
wird im nächsten Kapitel zunächst auf die Geschichte von AMT-Systemen eingegangen.
Außerdem wird erläutert, wie frühere AMT-Systeme funktionierten und aussahen.