\usepackage{csquotes}\section{KI-Modelle in AMT-Systemen}
\label{sec:ki_integration}
KI-Systeme haben in den letzten Jahren stark an Beliebtheit gewonnen.
AMT-Systeme bilden da keine Ausnahmen.
Vor allem durch die integration von CNNs und RNNs konnten AMT-Systeme zahlreiche Prozesse deutlich verbessern
und neue Errungenschaften in dem Forschungsgebiet erzielen.
Es gibt jedoch auch weitere wichtige KI-Modelle die in AMT-Systemen häufig genutzt werden
oder zur integration in Planung stehen.
In diesem Kapitel werden genau diese KI-Modelle stärker behandelt
und deren Aufgaben in der automatischen Musik transkription weiter erläutert.

\subsection{Convolutional Neural Networks}
CNNs sind neuronale Netze, welche besonders gut räumlich strukturierte Daten analysieren können.
Deshalb werden diese vor allem in der Analyse von Bildern genutzt.
Sie sind in der Lage, den Inhalt eines Bildes präzise zu analysieren und darin enthaltene Objekte zuverlässig zu identifizieren.
Auch KIs wie ChatGPT nutzen eine verbesserte Form von CNNs, um Bilder zu analysieren.
Im Fall der Musiktranskription wird als Input Bild ein Spektrogramm verwendet.
Spektrogramme können ähnlich wie zweidimensionale Bilder gehandhabt werden,
da auf diesen auch alle wichtigen Daten des Inputs Audiosignals zu finden sind.
CNNs bestehen aus mehreren verschiedenen Layern.
Einfache CNN Modelle bestehen nur aus zwei bis fünf Layern,
wobei komplexere CNNs aus über Tausenden Layern bestehen können.
In AMT-Systemen haben die meisten CNNs etwa drei bis zehn Layer.
Diese Layer sind Convolutional Layer, Activation Layer (ReLU), Pooling Layer,
Batch Normalization, Dropout Layer und Upsampling.
Mit jedem Layer kann ein CNN immer abstraktere Merkmale erkennen.
Außer dem Convolutional Layer und Activation Layer sind die anderen Layer jedoch nicht unbedingt notwendig.
Eine Arbeit, die die Stärke von CNN-Modellen in AMT-Systemen sehr gut darstellt, heißt \enquote{Onsets and Frames} \cite{hawthorne2017onsets}.
In dieser Arbeit werden direkt zwei verschieden spezialisierte Teilnetzwerke für Onsets und Sustain der Noten behandelt.
Mit diesem System wird die Entwicklung des Forschungsgebietes illustriert.
Insbesondere für polyphone Klaviertranskription ist dieses AMT-System ausgezeichnet.
Im Folgenden werden die verschiedenen Layer eines CNNs, in einem AMT-System, erläutert.

\subsubsection{Layer eines CNNs}
In dem Convolution Layer werden Filter verwendet.
Filter sind 2D-Matrizen, die aus trainierbaren Gewichten bestehen.
Ein Filter deckt jeweils einen Eingabebereich des Inputbildes ab.
Je nach Aufgabenfeld und Rechenaufwand  unterscheidet sich die größe eines Filters.
Ein 3x3 Pixel Filter ist der am weitesten verbreitete Standard.
Je größer ein Filter ist, desto mehr Rechenaufwand wird benötigt.
In AMT-Systemen werden häufig 5x3 oder 7x3 Pixel Filter genutzt,
da somit mehr zeitlicher Kontext als Frequenzkontext abgedeckt wird.
Jeder auf das Bild angewendete Filter wird über die gesamte Eingabe verschoben und analysiert dabei nacheinander alle Bildbereiche.
Ein Skalarprodukt aus Filter und Eingabebereich beschreibt dann einen Aktivierungswert.
Aus allen Aktivierungswerten eines Filters entsteht eine Feature Map.
Beim Vergleich von Aktivierungswerten können Patterns und Eigenschaften erkannt werden.
In der Musiktranskription werden dadurch Onsets, Sustains oder harmonische Verläufe herausgefiltert.
Onsets werden beispielsweise erkannt, wenn es eine plötzliche Energieänderung gibt.
Am Ende entsteht ein 3D-Tensor, welcher alle Feature Maps beinhaltet.

Die Batch Normalization sorgt dafür, das die Aktivierungswerte normalisiert werden.
Jede Feature Map wird dabei einzeln normalisiert.
Dadurch wird Rechenleistung eingespart.
Zudem lassen sich im Training durch Mini-Batches mehrere Spektrogramme gleichzeitig durch die CNN-Struktur leiten.
Dadurch wird das Training schneller und Ergebnisse werden früher erreicht.

Es kann passieren das die Summe eines Convolution Layers negativ ist.
Dies kann passieren, wenn stärker gewichtete Filter einen negativen Aktivierungswert herausgeben.
Negative Werte können zu Informationsverlust, von Eigenschaften des Musikstückes, führen.
Um das zu vermeiden werden im Activation Layer, meistens mit der ReLU Funktion,
alle negativen Aktivierungswerte auf 0 gesetzt.
So kann das Netz nichtlineare Beziehungen modellieren.

Als Nächstes wird mit dem Pooling Layer der Rechenaufwand verringert.
Dieser nimmt jede Feature Map einzeln und reduziert ihre Matrix zu einer kleineren, standardmäßig 2x2, Matrix.
Das erfolgt, indem sich der Pooling Layer zunächst eine
gesamte Feature Map nimmt und diese dann in kleinere Blöcke aufteilt.
Es gibt entweder Max-Pooling oder Average-Pooling.
Je nachdem welche Methode gewählt wird, wird immer der höchste Wert oder der durchschnittliche Wert extrahiert.
Der extrahierte Wert von jedem Block wird nun in die reduzierte Feature Map zurückgeführt.
Dadurch wird nicht nur Rechenaufwand reduziert, sondern auch Überanpassung vermieden.
Wenn das System jeden kleinsten Wert berücksichtigt, passt es sich zu sehr an die Trainingsdaten an
und kann womöglich andere Daten nicht mehr richtig analysieren.

Der Dropout Layer ist, im Gegensatz zu den anderen Layern, nur im Training relevant.
Er schaltet zufällig Neuronen aus,
sodass sich Neuronen nicht ausschließlich auf bestimmte andere Neuronen verlassen können.
Somit wird das gesamte neuronale Netzwerk robuster und vielseitiger.

Upsampling ist das Gegenteil von dem Pooling Layer.
Anstatt die Feature Maps zu reduzieren, werden diese wieder hochskaliert.
Dadurch lassen sich bestimmte Features wieder zeitlich präziser bestimmen,
da die Zeitfenster wieder genauer zum originellen Audiosignal passen.
Meist wird dieser Layer jedoch weggelassen, da er häufig nicht sehr relevant für AMT-Systeme ist.

Der Output eines CNNs ist ein 3D-Tensor:
\[
\text{CNN-Ausgabe} \in \mathbb{R}^{T \times F \times C}
\]
\begin{itemize}
\item $T$: Anzahl der Zeitfenster (Frames) im Spektrogramm.
\item $F$: Frequenzachsenlänge.
\item $C$: Anzahl der Filter des CNNs.
\end{itemize}

In der folgenden Darstellung wird ein CNN dargestellt, mit deren gegebenen Layern.
Als Input wird beispielhaft ein CQT-Spektrogramm verwendet.
Ein Kernel extrahiert einen lokalen Eingabebereich.
Durch die Convolution-Layer und ReLU-Layer werden die Merkmale extrahiert und somit entstehen Feature Maps.
Diese Feature Maps werden durch Batch Normalization normalisiert und anschließend durch Pooling-Layer sukzessive reduziert.
Darauf folgend werden diese Merkmale vektorisiert und durch ein Fully Connected Layer klassifiziert.
Am Ende entsteht eine Wahrscheinlichkeitsverteilung über mögliche Klassen.
In diesem Beispiel werden als Output Tonhöhen vorhergesagt.
Das gleiche CNN könnte aber auch andere musikalische Merkmale extrahieren, je nachdem wie dieses trainiert wurde.
Die Darstellung veranschaulicht somit die Merkmalsextraktion mithilfe eines CNNs.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Graphics/CNN}
    \caption[CNN Struktur]{Visualisierung einer CNN Struktur zur automatischen Musiktranskription. Eigene Darstellung in Anlehnung an  \cite{shahriar2020cnn}.}
    \label{fig:cnn-amt}
\end{figure}

Üblicherweise schließt sich, in einem AMT-System, nach einem CNN ein RNN an.
Dieses kann jedoch nur 1D-Vektoren verarbeiten und nicht einen 3D-Tensor.
Dieser 3D-Tensor wird deshalb vor der Übergabe durch eine Flattening-Operation
zeitschrittweise in 1D-Vektoren umgewandelt.
Dabei wird der Vektor mit einer Dimension von F × C erhalten.
Diese Vektoren werden dann an das folgende RNN weitergeleitet.

\subsubsection{Geschichtliche Einordnung von CNNs}
CNNs finden ihren Ursprung im Jahre 1979.
Die erste Architektur für CNNs wurde, unter dem Namen \enquote{Neocognitron}, veröffentlicht \cite{fukushima1980neocognitron}.
Das Neocognitron kann zwar als Vorläufer moderner Convolutional Neural Networks betrachtet werden,
unterschied sich jedoch wesentlich von späteren CNN-Architekturen.
Es dauerte weitere 10 Jahre bis das erste vollwertige Convolutional Neural Network veröffentlicht wurde \cite{lecun1989backpropagation}.
Dieses CNN Modell unterscheidet sich speziell in drei bestimmten Punkten zu dem Vorgänger Neocognitron.
LeCun integriert in seinem CNN Gradientenlernen durch Backpropagation,
wodurch das gesamte Netzwerk erstmals auf ein gemeinsames Ziel zu trainieren konnte.
Neocognitron besaß zudem, im Gegensatz zu LeCuns CNN,
keine Gewichtsverteilung mithilfe von Filtern, wie es in heutigen CNN Modellen Standard ist.
Das Neocognitron hatte zudem keine praktische Anwendung.
Es stellte zwar ein theoretisch wegweisendes Architekturkonzept dar,
doch die damaligen technischen Rahmenbedingungen erschwerten eine praktische Umsetzung.
Für AMT-Systeme wurden CNNs erst etwa ab dem Jahre 2015 relevant \cite{sigtia2016end}.
In dieser Arbeit wurden erstmals polyphone Musikstücke mithilfe von CNNs und weiteren KI-Modelle, transkribiert.
Seitdem sind CNNs ein wichtiger Bestandteil der modernen automatischen Musiktranskription.

\subsection{Recurrent Neural Networks}
RNNs sind neuronale Netze, welche entworfen wurden um Daten mit zeitlicher Struktur zu verarbeiten.
Dabei ist die Besonderheit von RNNs das diese ein Gedächtnis haben.
Wenn in einem AMT-System eine bestimmte Tonabfolge gespielt wurde,
kann sich das RNN diese merken und dementsprechend die fortlaufenden Ausgaben anpassen.
Dies passiert dank den Hidden States.
Diese stellen das Gedächtnis des RNNs dar und sind einer der wichtigsten Faktoren in einem RNN\@.
Die Hidden States werde im nächsten Abschnitt ausführlicher beschrieben.
Dieses Prinzip ist in der Musiktranskription sehr hilfreich,
da jede Note stark von den vorherigen gespielten Noten abhängt.
Takt, Rhythmus, Harmonie und die Melodie eines Musikstückes sind alles gute Beispiele,
warum diese sequenzielle Abfolge so passend in AMT-Systemen ist.

RNNs haben in der automatischen Musiktranskription mehrere Aufgabenfelder.
Je nachdem wie das RNN trainiert wird, bewältigt dieses alle Aufgaben oder nur einen Teil.
Diese Aufgaben bestehen aus Frame-Glättung (Temporal smoothing), Kontext-Modellierung (Temporal context modeling),
Feature-Zusammenführung (Sequential integration of acoustic features)
und Ausgabevorbereitung (time-distributed output classification).
Diese Aufgaben werden auf jedes Zeitfenster, auf jeden 1D-Vektor des CNNs, angewendet.
Doch bevor der Fokus auf diese einzelnen Aufgaben gelegt werden kann, muss zunächst geklärt werden, was Hidden States sind.

\begin{description}[style=nextline]
\item[Hidden States]\label{itm:hidden}
Hidden States sind das grundlegende Prinzip, warum RNNs funktioniere.
Sie stellen das Gedächtnis des RNNs dar und helfen somit
anderen Modulen Vorhersagen über bestimmte musikalische Eigenschaften zu treffen.
Für jeden 1D-Vektor, den das CNN liefert, wird ein Hidden State erstellt.
Diese werden sequenziell, mit Abhängigkeit zum vorherigen Hidden States, definiert.

Die folgende Gleichung beschreibt, wie der aktuelle Hidden State $h_t$ aus dem Eingabevektor $x_t$ und dem vorherigen Hidden State $h_{t-1}$ berechnet wird.
Der vorherige Hidden State $h_{t-1}$ repräsentiert nicht nur den unmittelbar vorausgehenden Zustand,
sondern auch alle davorliegenden, da diese in die Berechnung jedes neuen Hidden States einfließen.
Die Funktion $f$ repräsentiert dabei eine nichtlineare Transformation, mithilfe der Gewichte des RNNs.
Somit wird sichergestellt, dass der neue Hidden State sowohl den aktuellen Eingabevektor $x_t$, als auch die vorherigen Hidden States $h_{t-1}$ berücksichtigt.
\[
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})
\]
\begin{itemize}
\item $\mathbf{x}_t$: Eingabevektor zum Zeitpunkt $t$.
\item $\mathbf{h}_{t-1}$: Vorheriger Hidden State.
\item $f$: Nichtlineare Transformationsfunktion.
\item $\mathbf{h}_t$: Neuer Hidden State.
\end{itemize}

Hidden States enthalten das Wissen der vorherigen Zeitfenster über zeitliche Muster, wie Sustain und Akkordstruktur.
Alleine können Hidden States keine eigene Entscheidung über bestimmte Noten fallen.
Dafür müssen andere Module die Informationen der Hidden States später richtig verwerten.
Jeder Hidden State hat eine Anzahl von Dimensionen, welche gleich der Anzahl der Neuronen im RNN ist.
\end{description}

\subsubsection{Grundstruktur eines RNNs}
Bei der Frame-Glättung bekommt das RNN als Input die Aktivierungswerte des Zeitfensters.
Es betrachtet diese mit Kontext zu den vorherigen Zeitfenstern, um falsche Vorhersagen auszuschließen.
Das CNN hat dem RNN schon Vorhersagen für bestimmte Noten gegeben.
Diese Vorhersagen könnten jedoch Fehler enthalten.
Zum Beispiel kann die Note C4 für den Frame 6 und 8 aktiv sein,
aber bei dem Frame 7 hat das CNN diese Note nicht als aktiv angesehen.
Es ist sehr unwahrscheinlich, das eine Note für nur einen Frame ausfällt.
Solche Arten von Fehlern verarbeitet das RNN über mehrere Zeitfenster hinweg, mithilfe seiner wiederkehrenden Struktur.
Dieser Vorgang wird auch als zeitliche Entfaltung bezeichnet.
Deshalb aktiviert das RNN den Ton C4 auch im Frame 7.

Die folgende Abbildung zeigt die Entfaltung eines einzigen RNN-Blocks über mehrere Zeitpunkte hinweg.
In der Abbildung steht $x$ für den 1D-Inputvektor eines CNNs,
$h$ für den Hidden State und dem somit verbundenen Gedächtnis des RNNs
und $o$ für den Outputvektor, mit dem weiter gerechnet wird.
Die Buchstaben $W$, $U$ und $V$ stellen dabei die verschiedenen
Gewichte \enquote{Input-Weight}, \enquote{Recurrent-Weight} und \enquote{Output-Weight} dar.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Graphics/rnn_unfold}
    \caption[Entfaltung eines RNN-Blocks]{Entfaltung eines RNN-Blocks  \cite{wikimediaRNN}.}
    \label{fig:rnn_block_unfold}
\end{figure}
Diese Abbildung illustriert folgende Formel.
Dabei steht $b$ für den Bias-Vektor des Hidden Layers und $c$ für den Bias-Vektor des Output-Layers.
\begin{align*}
h_t &= \tanh\left( W \cdot x_t + U \cdot h_{t-1} + b \right) \\
o_t &= \sigma\left( V \cdot h_t + c \right)
\end{align*}

Neben der Frame-Glättung wird die Kontext-Modellierung auf den gegebenen Input angewendet.
Als Input bekommt diese auch die 1D-Vektoren des CNNs.
In der Kontext-Modellierung werden größere zeitliche Zusammenhänge betrachtet.
So kann die Kontext-Modellierung, mithilfe des Hidden States,
den Notenverlauf oder auch die Länge einer Note vorhersehen.
Je nach den Trainingsdaten ist es zum Beispiel üblicher das auf C4 ein D3 folgt,
was durch die Kontext-Modellierung angepasst wird.
Dieser musikalische Kontext ist aber immer stark von der Musikrichtung und von den Musikern abhängig,
welche in den Trainingsdaten vorhanden sind.
Bei Jazz und Pop oder Johann Sebastian Bach und Taylor Swift unterscheidet sich der Stil der Tonabfolge extremst.
Zudem wird Onset, Sustain und Offset stabilisiert.
Durch vorherige Beispiele weiß die Kontext-Modellierung,
wie lange eine bestimmte Note andauern wird und kann so das Offset der Note einschätzen.
Als Output entstehen kontextabhängige Vektoren.
Sie haben die gleiche Struktur wie die 1D-Vektoren vom CNN, sind aber entsprechend dem Kontext angepasst.

Die Feature-Zusammenführung ist der letzte wichtige interne Schritt eines RNNs.
Bei diesem werden aus lokalen alleinstehenden Informationen eines Zeitfensters, konkrete musikalische Ereignisse.
Dadurch schreibt das RNN Ereignisse wie Akkorde, Noten, Onsets und weitere heraus.
Dafür muss die Feature-Zusammenführung sich die einzelnen 1D-Vektoren als eine Folge von Events anschauen.
Dies passiert über den Hidden State.
Das wird vor allem wichtig, wenn später eine MIDI-Datei ausgegeben werden soll,
da in dieser auch die einzelnen musikalischen Ereignisse aufgeschrieben sind.
Als Output entstehen kontextreiche Vektoren.
Diese Vektoren sind, durch die vorherigen Module angepasste und verbesserte, Hidden States.

Die Ausgabevorbereitung ist der letzte Schritt des RNNs,
wodurch nun die gesammelten Daten zu echten musikalischen Ereignissen zusammengefügt werden.
Als Input werden die, durch die Feature-Zusammenführung verbesserten, Hidden States genutzt.
Zunächst werden diese durch ein Fully Connected Layer geschickt.

Ein Fully Connected Layer ist ein Klassifikator, welcher den Hidden States auf eine gewünschte Dimension bringt.
Wenn zum Beispiel Klaviertasten vorhergesagt werden sollen, werden alle Hidden States mit 88 Dimensionen ausgestattet.
Bei MIDI-Dateien wären es 128 Dimensionen.
Die Werte, welche aus dem Fully Connected Layer stammen, sind jetzt noch nicht richtig interpretierbar.
Um diese als konkrete und normalisierte Wahrscheinlichkeiten darstellen zu können,
wird eine Aktivierungsfunktion eingesetzt.
Mit beispielsweise der Sigmoid-Funktion als Aktivierungsfunktion
können alle Werte normiert in einem Bereich zwischen 0 und 1 gebracht werden.
Sagen wir, wir wollen jetzt die gespielten Klaviertasten vorhersagen.
Dann hat jeder Hidden State für alle Klaviertasten einen eigenen Wert mit einer Wahrscheinlichkeit,
dass diese Taste zu dem gewählten Moment gespielt wurde.
Zum Schluss muss jetzt ein Threshold bestimmt werden.
In polyphonen Musikstücken können immer mehrere Noten gleichzeitig erklingen,
weshalb nicht einfach die Note mit der höchsten Wahrscheinlichkeit ausgewählt werden kann.
Deshalb wird ein Threshold genutzt, zum Beispiel bei 50\%,
welcher bestimmt, wie viel Prozent eine Note braucht, um als aktiv zu gelten.
Durch Postprocessing können einige Eigenschaften wie Rauschen noch herausgefiltert werden.
Postprocessing ist jedoch nicht relevant für den KI-Ablauf.
Wenn das Ergebnis zufriedenstellend ist, kann es in das gewünschte Output-Format,
standardmäßig MIDI-Dateien, eingefügt werden.

Heutzutage sind RNNs nur die grundlegende Struktur.
Basierend auf dieser gibt es einige verbesserte Systeme,
welche Aktiv in AMT-Systemen und anderen KI-Systemen genutzt werden.
Zwei dieser Systeme sind Long Short-Term Memory's (LSTM) und Bidirektionale RNNs (BiRNN).
Diese werden im folgendem Abschnitt ausführlicher erklärt.

\subsubsection{Long Short-Term Memory}
LSTMs sind verbesserte RNNs.
Diese kontrollieren durch Gates besser, welche Daten sie wirklich in den Hidden State speichern möchten.
Dadurch lässt sich das neuronale Netz noch weiter an die gewünschten Ansprüche anpassen.

In einem einfachen RNN werden alle Daten, egal ob sinnvoll oder nicht, miteinander in dem Hidden State kombiniert.
Somit kann sich das RNN langfristig schwieriger Information merken.
Wenn zum Beispiel im 2.\ Hidden State ein Onset erkannt wurde, kann der 20.\ Hidden State
sich das schlechter merken, da viele andere Informationen mitgeschrieben wurden.
LSTMs lösen dieses Problem mit Forget, Input und Output Gates und dem Cell State.
Der Cell State stellt das Langzeitgedächtnis des LSTMs dar.
Er berechnet sich aus den drei Gates.
Das Forget Gate bestimmt, welche Informationen aus dem vorherigen Cell State gelöscht werden sollen.
Das Input Gate bestimmt, welche neuen Inhalte aus dem neuen Zeitfenster aufgenommen werden.
Dabei ist der Cell-candidate die Datenmenge, welche zum Speichern, durch das Input Gate, vorgeschlagen wird.
Das Output Gate bestimmt, welcher Teil des Cell States zu dem neuen Hidden State hinzugefügt wird.
\begin{align*}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &&\text{(Forget Gate)} \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &&\text{(Input Gate)} \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &&\text{(Output Gate)} \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \quad &&\text{(Cell-candidate)} \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad &&\text{(Cell State)} \\
h_t &= o_t \odot \tanh(c_t) \quad &&\text{(Aktueller Hidden State)}
\end{align*}
Dadurch kann sich ein LSTM die verschiedenen musikalischen Ereignisse,
über das gesamte Audiosignal, besser im Zusammenhang merken.

Folgende Abbildung zeigt einen Zeitschritt in einem LSTM.
Dabei sind die Komponenten gleich benannt wie in der Abbildung-\pref{fig:rnn_block_unfold}.
Das $c$ steht für den Cell-State.
Intern in der \enquote{LSTM Unit} stehen die Buchstaben $F$, $I$ und $O$ für die Gates:
\enquote{Forget Gate, Input Gate und Output Gate}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Graphics/LSTM_timestep}
    \caption[LSTM Zeitschritt]{Zeitschritt eines LSTMs  \cite{wikimediaRNN}.}
    \label{fig:lstm_timestep}
\end{figure}

\subsubsection{Gated Recurrent Units}
GRUs sind, neben LSTMs, eine weitere spezielle Art von RNNs \cite{chung2014empirical}.
Sie wurden erfunden, um bestimmte Probleme von RNNs zu lösen.
Insbesondere das Problem des \enquote{Vanishing Gradients} sollten GRUs lösen.
GRUs steuern mithilfe von Gates, welche Informationen gemerkt und welche vergessen werden sollen.
Sie sind, im Gegensatz zu LSTMs, eher für kleinere Aufgaben geeignet.
Dafür sind sie weniger fehleranfällig und haben ein schnelleres Training.

Ein GRU besitzt zwei Gates.
Sie haben, genau wie bei LSTMs, die Aufgabe das Gedächtnis des KI-Models zu verwalten.
Das Reset Gate schaut sich den alten Hidden State an und entscheidet,
wie viel von diesem Wissen in den neuen Hidden State mit einfließt.
Das Update Gate hingegen sieht den aktuellen Hidden State und überlegt,
welche Informationen davon in das Langzeitgedächtnis mit einbezogen werden.
Danach führt das Update Gate die beiden überarbeiteten Hidden States zusammen zum neuen Hidden State.

Das Problem des Vanishing Gradients betrifft das Gedächtnis neuronaler Netze und tritt insbesondere während der Trainingsphase auf.
Bei der Backpropagation werden die Gradienten mit jedem durchlaufenen Layer kleiner, sodass betroffene Layer im Netzwerk kaum noch dazulernen.
Dies liegt daran, dass Aktivierungsfunktionen Werte im Bereich zwischen 0 und 1 zurückgeben und ihre Ableitungen ebenfalls kleiner als 1 sind.
Da die Gradienten bei jedem Schritt mit diesen Ableitungen multipliziert werden, schrumpfen sie exponentiell mit zunehmender Netzwerktiefe.
Infolgedessen verliert das Modell die Fähigkeit, Informationen über längere Zeiträume hinweg zu speichern.
Bei GRUs wird das Problem des Vanishing Gradients durch die Gates gelöst.
Sie stellen eine gewichtete Mischung aus altem und neuem Hidden State dar.
Dadurch wird der Gradient nicht ständig verkleinert
und wichtige Informationen können über einen langen Zeitraum gespeichert werden.

Folgende Abbildung zeigt einen Zeitschritt in einem GRU.
Die Komponenten sind gleich benannt wie in der Abbildung-\pref{fig:rnn_block_unfold}.
Intern in der \enquote{GRU Unit} stehen die Buchstaben \enquote{R und Z} für die Gates:
\enquote{Reset Gate und Update Gate}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Graphics/GRU_timestep}
    \caption[GRU Zeitschritt]{Zeitschritt eines GRUs  \cite{wikimediaRNN}.}
    \label{fig:gru_timestep}
\end{figure}

\subsubsection{Bidirektionale RNNs}
Um ein noch besseres Ergebnis zu erhalten, kann auch ein bidirektionales RNN genutzt werden.
Dieses besteht aus zwei RNNs.
Eines liest die Zeitfenster von vorne und das andere von hinten ab.
Dadurch entsteht die doppelte Menge an Hidden States und die gesamte Vorhersage wird robuster.

Die folgende Formel beschreibt die Konstruktion des Hidden States $\mathbf{h}_t$ in einem bidirektionalen RNN.
Dabei setzt der neue Hidden State sich aus zwei Teilen zusammen.
Dem Vorwärts-Hidden-State $\overrightarrow{\mathbf{h}}_t$,
der die Informationen aus der Vergangenheit bis zum Zeitpunkt $t$ enthält,
und dem Rückwärts-Hidden-State $\overleftarrow{\mathbf{h}}_t$,
der die Informationen aus der Zukunft bis $t$ einbezieht.
Diese beiden Hidden States werden zusammengefügt,
sodass der gewonnene Hidden State Informationen über das gesamte Musikstück enthält.
\[
\mathbf{h}_t = \left[ \overrightarrow{\mathbf{h}}_t \ ;\ \overleftarrow{\mathbf{h}}_t \right]
\]
\begin{itemize}
\item $\overrightarrow{\mathbf{h}}_t$: Vorwärts-Hidden-State.
\item $\overleftarrow{\mathbf{h}}_t$: Rückwärts-Hidden-State.
\item $\left[ \cdot \ ; \ \cdot \right]$: Zusammenfügung der beiden Hidden States.
\item $\mathbf{h}_t$: Vollständiger Hidden State zum Zeitpunkt $t$.
\end{itemize}

Auch in AMT-Systemen sind bidirektionale RNNs sehr hilfreich,
da musikalische Ereignisse auch eine klar verständliche Abhängigkeit von der Zukunft in die Vergangenheit haben.
Das gleiche Prinzip lässt sich auch auf LSTMs oder GRUs anwenden,
sodass beispielsweise ein bidirektionales LSTM entsteht.
Bidirektionale RNNs sind noch robuster, aber dafür ist der Rechenaufwand bei weitem höher.
Ein bidirektionales RNN findet sich auch in folgender Arbeit \cite{hawthorne2017onsets}.

\subsubsection{Geschichte und Weiterentwicklung von RNNs und ihren Varianten}
Die Geschichte von Recurrent neural networks reicht bis in das Jahr 1990 zurück \cite{elman1990finding}.
In dieser Arbeit wurde erstmals die Idee der Rückkopplung eingebaut,
wodurch der Output eines Neurons als zusätzliche Eingabe im nächsten Zeitfenster genutzt wird.
Das resultierte später in den Hidden States.
Diese Arbeit baute den Grundstein für alle weiterführenden RNNs.
Ein weiterer früher Ansatz wurde bereits 1986 als technischer Bericht vorgestellt und 1997 publiziert \cite{jordan1997serial}.
Er führte sogenannte Kontexteinheiten ein, bei denen der Output eines Neurons in das folgende Zeitfenster rückgekoppelt wird.
Damit wurde ein alternativer Mechanismus zur Sequenzmodellierung vorgeschlagen,
der sich von Elmans Rückkopplung aus dem Hidden Layer unterschied und wichtige Grundlagen für spätere Varianten von RNNs schuf.
Zudem wurde, im Jahre 1997, das erste Bidirektionale RNN \cite{schuster1997bidirectional}
und das erste LSTM erfunden \cite{hochreiter1997long}.
Erst über ein Jahrzehnt später wurde das erste GRU entwickelt \cite{chung2014empirical}.
RNNs fanden ihren Weg in die automatische Musiktranskription, fast zeitgleich zu CNNs, im Jahre 2015 \cite{sigtia2015hybrid}.
Für LSTMs dauerte die Einbindung in AMT-Systeme etwas länger.
Erst im Jahre 2016 wurde ein LSTM in einem AMT-System eingefügt \cite{sigtia2016end}.
Dahingegen wurde das erste Bidirektionale RNN erst Ende 2017 in ein AMT-System integriert \cite{hawthorne2017onsets}.
Noch ein Jahr später wurde das erste Mal auch ein GRU in einem AMT-System genutzt \cite{jung2018adaptive}.

\subsection{Transformers}
Transformer sind eine, von Google entwickelte Deep-Learning Architektur \cite{vaswani2017attention},
die mit dem Prinzip der Self-Attention, zur natürliche Sprachverarbeitung genutzt wird.
Auch Transformer verarbeiten die Daten, wie ein LSTM, sequentiell.
Jedoch wurde zu dieser Verarbeitung noch der Attentionmechanismus hinzugefügt,
welcher der grundlegende Baustein eines Transformer-Modells ist.
Die Architektur eines Transformers besteht grundlegend aus drei Bausteinen.
Diese sind Input Embedding mit Position Encoding, Self-Attention Layer und Feedforward Layer.
Input Embedding und Position Encoding werden nur einmalig am Anfang des Transformers durchgeführt.
Dahingegen werden Self-Attention und Feedforward mehrmals hintereinander aufgerufen.
Dies passiert grundsätzlich 6-, 12- oder 24-mal, wodurch das Transformer-Modell an Layern gewinnt.
Dieser Schritt wird so oft wiederholt, damit der Transformer immer komplexere musikalische Strukturen erkennen kann.
Als Nächstes werden die einzelnen Schritte eines Transformers näher erläutert.

\subsubsection{Input Embedding und Position Encoding}
Zunächst müssen die Daten, welche der Transformer verarbeiten soll in das richtige Format für diesen umgewandelt werden.
Dafür ist das Input Embedding zuständig.
Tokens sind einzelne Datenpunkte wie beispielsweise kurze Wörter wie \enquote{hallo}.
Wenn zum Beispiel bei ChatGPT die Anfrage \enquote{Wie alt sind die Planeten unseres Sonnensystems} gestellt wird,
dann wird zwar das Wort \enquote{alt} als ein Token gespeichert, aber längere Wörter wie \enquote{Sonnensystem}
werden aufgeteilt in \enquote{Sonn} und \enquote{ensystem}.
Je nach Tokenizer-Version, die der jeweilige Transformer nutzt, kann dies jedoch abweichen.
Bei diesem Beispielsatz werden, alleine für die Wörter, neun Tokens genutzt.
Tokens können auch einzelne Satzzeichen oder Symbole sein.
Für verschiedene Transformer Modelle gibt es immer eine maximale Tokenanzahl,
bei GPT-3.5 sind es zum Beispiel ungefähr 4000 Tokens.
Diese Tokens sind für den Input und Output.
Heißt, wenn der Input zu viele Tokens nutzt, gibt es weniger Tokens für den Output.
Dies ist jedoch häufig, wie an der Menge der Tokens für unseren Beispielsatz zu sehen ist, kein größeres Problem.
In AMT-Systemen stellen Tokens Zeitfenster (Frames) oder Musik-Events (Onsets etc.) dar.
Jedoch können diese von einem Transformer nicht verarbeitet werden,
weshalb sie durch Input Embedding erstmals in Vektoren umgewandelt werden.
Dies passiert über eine Embedding-Matrix, welche die einzelnen Tokens in den Vektorraum einbettet.
\[
\displaystyle
\text{Embedding-Matrix} \in \mathbb{R}^{\left( \text{Maximale Tokenanzahl } \times \text{ Embedding-Dimension} \right)}
\]
Tokens, welche ähnliche musikalische Eigenschaften speichern, werden im Vektorraum näher beieinander gespeichert.
Das Modell kann die Tokens, durch die gesammelten Daten im Training, richtig einordnen.

Nun sieht der Transformer die Tokens trotzdem nur als zahlreiche Vektoren, ohne Rheinfolge, an.
Um den Tokens einen Sinn zu geben, müssen diese eine explizite Position erhalten.
Das wird durch Position Encoding gemacht.
Durch Sinus- und Cosinunsfunktionen wird für jeden Token ein eindeutiger Positionsvektor berechnet.
Dies geschieht, indem für jede Dimension, die unsere Tokens besitzen, eine Funktion mit unterschiedlicher Frequenz gebildet wird.
Bei geraden Dimensionen wird eine Sinusfunktion genutzt und bei ungeraden eine Cosinusfunktion.
So erhält jede Position einen eindeutigen Positionsvektor,
während nahe Positionen ähnliche Encodings und somit geringeren Abstand zueinander besitzen.
Dadurch kann der Transformer die relative Position, verschiedener Tokens,
gut miteinander vergleichen und zugleich wiederkehrende Muster erkennen.
Danach wird der Positionsvektor dem Tokenvektor aufaddiert.
Moderne Transformer besitzen manchmal auch gelernte Position Embeddings,
wodurch die Positionen schon durch das Training bekannt sind.
Diese Vektoren werden, als Input, weiter an den Transformer geleitet.

\subsubsection{Self-Attention Layer}
Jeder Input-Vektor wird einzeln behandelt.
Durch Self-Attention kann sich jeder Vektor merken, welche anderen Vektoren für ihn relevant sind.
Zunächst wird jeder Input-Vektor in drei verschiedene Vektoren umgewandelt.
Diese Vektoren heißen \enquote{Query}, \enquote{Key} und \enquote{Value}.
\begin{itemize}
  \item \textbf{Query ($Q$)}: Bestimmt, auf welche Informationen aus anderen Tokens das Modell aktuell achten möchte.
  \item \textbf{Key ($K$)}: Zeigt anderen Tokens, was dieser Input-Vektor anderen Tokens, für Informationen, anbieten kann.
  \item \textbf{Value ($V$)}: Enthält die tatsächlichen Informationen des Input-Vektors.
\end{itemize}
Als Nächstes wird der \enquote{Attention Score} berechnet.
Durch diesen wird die Kompatibilität zu anderen Tokens berechnet.
Mit Kompatibilität ist dabei die Ähnlichkeit zwischen einem Query- und einem Key-Vektor gemeint.
Diese gibt an, wie stark ein Token im Verhältnis zu einem anderen berücksichtigt werden soll.
Als Ergebnis entsteht für jeden Token eine Matrix.
Jeder Wert in dieser Matrix steht für die Ähnlichkeit zwischen zwei verschiedenen Tokens.
Diese Werte sind die Attention Weights.
Durch Softmax werden diese Werte normalisiert.
Als Letztes werden diese Attention Weights mit den Value-Vektoren verbunden.
\[
\text{Output} = \text{Attention Weights} \times \text{Value Vektoren}
\]
Nun können die einzelnen Tokens sich besser auf die Tokens fokussieren, welche wirklich wichtig für deren Ergebnis sind.

\subsubsection{Feedforward Layer}
Der Feedforward Layer stammt von der Idee eines \enquote{Feedforward Neural Networks} (FNN).
Dieses neuronale Netz verläuft immer nur in eine Richtung und hat keine Rückkopplung.
Das heißt jeder Token wird für sich alleine, nacheinander, umgeschrieben.
Das funktioniert, da die jeweilige Gewichtung schon im Self-Attention Layer stattgefunden hat.
Im Feedforward Layer passiert dann folgendes.
Zunächst werden die Vektoren, welche aus dem Self-Attention Layer stammen,
mit einer Gewichtsmatrix auf eine höhere Dimension transformiert.
Durch die Erhöhung der Dimension bekommt das Netzwerk mehr Freiraum Informationen voneinander zu trennen
und komplexere Zusammenhänge zu modellieren.
Durch eine Aktivierungsfunktion, zum Beispiel ReLU, erlernt das Modell jetzt nichtlineare Abhängigkeiten.
Diese Aktivierungsfunktion gehört zu dem Hidden Layer des FNN.
Ein FNN kann mehrere Hidden Layer besitzen, welche alle die Input-Werte in irgendeiner Form verändern.
Als Nächstes wird der Vektor wieder auf seine ursprüngliche Dimension zurückprojiziert,
wodurch nur die wichtigsten Informationen erhalten bleiben.
Mathematisch sieht der Feedforward Layer folgendermaßen aus:
\[
y_t = \text{FFN}(x_t) = \left( x_t W_1 + b_1 \right) \xrightarrow{\text{ReLU}} W_2 + b_2
\]
\begin{itemize}
  \item $x_t$: Eingabevektor des Tokens.
  \item $W_1, W_2$: Gewichtsmatrizen, zuständig für Dimensionserweiterung und -reduktion.
  \item $b_1, b_2$: Bias-Vektoren der jeweiligen linearen Projektionen.
  \item $\left( x_t W_1 + b_1 \right)$: Ergebnis der ersten linearen Projektion.
  \item $\text{ReLU}$: Aktivierungsfunktion zur Einführung von Nichtlinearität.
  \item $y_t$: Ausgabewert des Feedforward Layers für das Token.
\end{itemize}

\subsubsection{Geschichtliche Einordnung von Transformern}
Das erste Transformer-Modell wurde im Jahre 2017 erfunden \cite{vaswani2017attention}.
Durch den Self-Attention Layer wurde die sequenzielle Modellierung von Daten revolutioniert.
Es dauerte noch lange bis der erste Transformer auch in AMT-Systemen genutzt wurde.
Das liegt an mehreren Gründen.
Einerseits gibt es im Gegensatz zu natürlichen Sprachverarbeitung weniger Datensätze, von denen die Modelle lernen könnten.
Transformer trainieren auf einem globalen Level und brauchen daher eine große Menge an Datensätzen.
Die Rechenkosten von Transformer Modellen sind auch weitaus höher als bei anderen Systemen, wie CNNs und RNNs.
Zudem lag der Fokus bei AMT-Systemen für eine sehr lange Zeit überwiegend bei CNN und RNN basierenden Systemen,
da diese Anwendungsfälle bekannter waren in AMT-Systemen.
Das größte Problem war jedoch wahrscheinlich die Anpassung.
Transformer Modelle waren einfach nicht dafür ausgelegt musikspezifische Daten zu verarbeiten.
In der Musiktranskription wird immer mit langen Sequenzen, ein gesamtes Musikstück, gearbeitet.
Spezielle Transformer Modelle für diesen Anwendungsfall mussten noch programmiert werden.
Die ersten Ansätze für Transformer in AMT-Systemen wurden zwischen den Jahren 2021 und 2023 erstellt.
Eines der ausschlaggebendsten Modelle war der Music Transcription Transformer MT3,
welcher vom Magenta-Team bei Google Brain erstmals im Jahre 2022 veröffentlicht wurde \cite{gardner2021mt3}.
Dieses spielt eine große Rolle in der Transformer-basierten automatischen Musiktranskription,
da es das erste weit verbreitete Multi-Task-Transkriptionsmodell ist.
Eine der Letzten bedeutenden Errungenschaften zu Transformern und Musiktranskription ist das
YourMT3+ Modell, indem die MT3-Architektur noch weiter verbessert wurde \cite{chang2024yourmt3+}.

\subsection{Potentielle KI-Modelle}
Bei der KI integration in AMT-Systemen werden überwiegend CNNs und RNNs oder Transformer Modelle wie MT3 verwendet.
Jedoch gibt es noch andere KI-Modelle, die in AMT-Systemen integriert werden können.
Diese gehen, bei der Musiktranskription, ganz anders vor als die vorgestellten Module.
Je nach KI übernehmen sie auch eine ganz andere Aufgabe.
Im folgenden Abschnitt werden einige von diesen, eher experimentellen, KIs vorgestellt.
Wir fangen dabei bei der am meisten erprobten KI an,
sodass die letzten KIs nur theoretisch, für die Musiktranskription, besprochen worden.

\subsubsection*{Variational Autoencoder}
Variational Autoencoder (VAE) ist ein KI-Modell, welches komplexe Daten in eine verdichtete Form überführt
und durch Wahrscheinlichkeiten bestimmte Muster vorhersagen kann \cite{kingma2019introduction}.
Dabei ist wichtig zu erwähnen das VAE kein alleinstehendes KI-Modell ist,
sondern eher als Zusatz gilt um bestimmte Bereiche zu verbessern.
In der Musiktranskription könnte ein VAE zum Beispiel bei der Mehrdeutigkeit von Musik helfen.
Wenn mehrere Instrumente spielen ist es schwer die genauen Noten herauszuschreiben.
Da VAEs, anders als CNNs oder RNNs, als Ausgabe keine eindeutigen Noten herausgeben,
sondern eine Wahrscheinlichkeit, könnte dies helfen die Entscheidung, welche Note gerade spielt, robuster zu gestalten.
Dadurch, dass VAEs nur eine wahrscheinliche Version der Musik aus den Daten extrahieren,
könnten damit auch kreative Variationen des Musikstückes gebildet werden.
Die Methode, welche in VAEs genutzt wird, fand Ihren Ursprung in folgendem Paper \cite{kingma2013auto}.

\subsubsection*{Graph Neural Network}
Graph Neural Network (GNN) ist ein KI-Modell, welches dazu dient Daten, mit Abhängigkeit voneinander, zu verarbeiten.
Diese Daten werden in einem Graphen aus Knoten und Kanten gespeichert.
Dabei hat jeder Knoten seine eigenen Daten, die er in jedem Rechenschritt mit seinen benachbarten Knoten austauscht.
Knoten sind benachbart, wenn sie durch Kanten verbunden sind.
Dadurch wird jeder Knoten im Netzwerk mit mehr Kontext ausgestattet.
Natürlich machen RNNs und Transformer etwas Ähnliches wie ein GNN, mit zum Beispiel Backpropagation.
Der Unterschied ist hier, das GNNs keine bestimmte Reihenfolge, wie Wissen weitergeleitet wird, haben.
Es ist ein anderer Ansatz harmonische Abhängigkeiten miteinander zu kombinieren \cite{wu2020comprehensive}.

\subsubsection*{Diffusion Model}
Diffusion Modelle arbeiten mit Rauschen.
Aus komplettem Rauschen bauen sie Schritt für Schritt ein Musikstück zusammen.
Dabei entscheiden sie pro Rechenschritt nur einen kleinen Teil des Musikstückes,
wodurch mehrdeutige Passagen in Musikstücken auch noch im späteren Transkriptionsprozess behandelt werden können.
In dem Projekt DiffRoll, aus dem Jahre 2022, wurde ein Diffusion Model auf ein AMT-System angewendet \cite{cheuk2022diffroll}.

\subsubsection*{Reinforcement Learning}
Reinforcement Learning benutzen ein Belohnungssystem, damit der Agent sich beim Training richtig anpasst.
Sobald der Agent ein bestimmtes Ziel erreicht oder eine Handlung ausführt, wird dieser dafür belohnt.
Die Agenten, welche am meisten Belohnungen erzielen, werden kopiert und in der nächsten Iteration eingesetzt,
bis ein Agent die gewünschte Aufgabe erfüllt.
Dieses Prinzip wird vor allem in Videospielen eingesetzt, wo es grundlegend eindeutige Ziele gibt.
In der Musiktranskription könnte dieses Prinzip folgendermaßen umgesetzt werden:
Der Agent erhält als Input eine Audiodatei.
Danach erzeugt er Noten nacheinander.
Durch eine andere KI oder vorgefertigte MIDI-Dateien steht ein Vergleichsdatensatz zur Verfügung.
Falls die gewählten Noten vom Agenten gleich dem Vergleichsdatensatz sind, bekommt der Agent Belohnungen.
So kann er sich selber musikalisches Wissen aneignen und Transkriptionen von anderen KIs nochmal korrekturlesen \cite{li2018music}.

\subsubsection*{Energy-Based Model}
Der letzte Ansatz ist ein Energy-Based Model (EBM).
Ein EBM arbeitet mit Energie anstatt von Wahrscheinlichkeiten \cite{lecun2006tutorial}.
Die verschiedenen Arten, wie das EBM das Musikstück transkribieren kann, haben alle jeweils ihre eigene Energie.
Dabei haben die besten Möglichkeiten immer die niedrigste Energie, wie bei dem Gradientenabstiegsverfahren.
Dies ist für AMT-Systeme interessant, da das EBM somit zahlreiche verschiedene Transkriptionsraten wählen könnte.
In mehr improvisierten Musikrichtungen, wie Jazz, bildet das eine größere Vielfalt.
Auch Musiktheorie lässt sich in das Modell einbauen,
wodurch aus einem Musikstück Unmengen an Remixen kreiert werden können.
Der Ansatz von EBMs in AMT-Systemen ist,
von den zusätzlich vorgestellten KI-Modellen, wahrscheinlich einer der Interessantesten.
Jedoch sind EBMs sowohl schwer zu trainieren, als auch sehr rechenintensiv.
AMT-Systeme sind ohnehin schon recht anspruchsvoll,
weshalb noch kein EBM-Modell in der automatischen Musiktranskription zum Einsatz kam.

